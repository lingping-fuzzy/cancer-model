{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 104884,
     "sourceType": "datasetVersion",
     "datasetId": 54339
    },
    {
     "sourceId": 8811582,
     "sourceType": "datasetVersion",
     "datasetId": 5300241
    },
    {
     "sourceId": 8858536,
     "sourceType": "datasetVersion",
     "datasetId": 5333050
    },
    {
     "sourceId": 9186077,
     "sourceType": "datasetVersion",
     "datasetId": 5552785
    },
    {
     "sourceId": 9494914,
     "sourceType": "datasetVersion",
     "datasetId": 5777592
    },
    {
     "sourceId": 10542425,
     "sourceType": "datasetVersion",
     "datasetId": 6523137
    }
   ],
   "dockerImageVersionId": 30747,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\n# import cv2\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms.functional as F\nimport matplotlib.pyplot as plt\n# Create a custom dataset class\nfrom torchvision.transforms.functional import to_pil_image\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\nimport os\nfrom glob import glob\nimport pandas as pd\n\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\n\nfrom torchvision import  transforms\nimport numpy as np\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-01T13:58:11.093553Z",
     "iopub.execute_input": "2025-05-01T13:58:11.094007Z",
     "iopub.status.idle": "2025-05-01T13:58:18.520395Z",
     "shell.execute_reply.started": "2025-05-01T13:58:11.093953Z",
     "shell.execute_reply": "2025-05-01T13:58:18.519169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "lesion_type_dict = {\n        'nv': 'Melanocytic nevi',\n        'mel': 'Melanoma',\n        'bkl': 'Benign keratosis-like lesions ',\n        'bcc': 'Basal cell carcinoma',\n        'akiec': 'Actinic keratoses',\n        'vasc': 'Vascular lesions',\n        'df': 'Dermatofibroma'\n    }",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:33:22.754464Z",
     "iopub.execute_input": "2024-10-20T13:33:22.754826Z",
     "iopub.status.idle": "2024-10-20T13:33:22.759787Z",
     "shell.execute_reply.started": "2024-10-20T13:33:22.754786Z",
     "shell.execute_reply": "2024-10-20T13:33:22.758768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "IMG_SIZE = 256\ntransform_train = transforms.Compose(\n[\n        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n        transforms.RandomHorizontalFlip(),  # Data augmentation: horizontal flip\n        transforms.RandomRotation(10),  # Data augmentation: random rotation\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\ntransform_val = transforms.Compose(\n[\n        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:33:20.504995Z",
     "iopub.execute_input": "2024-10-20T13:33:20.505739Z",
     "iopub.status.idle": "2024-10-20T13:33:20.512521Z",
     "shell.execute_reply.started": "2024-10-20T13:33:20.505706Z",
     "shell.execute_reply": "2024-10-20T13:33:20.511535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "def _get_df_data_():\n",
    "    train_df = pd.read_csv('/kaggle/input/ucidatasplits/train_df.csv').drop(columns=['Unnamed: 0'])\n",
    "    val_df = pd.read_csv('/kaggle/input/ucidatasplits/val_df.csv').drop(columns=['Unnamed: 0'])\n",
    "    test_df = pd.read_csv('/kaggle/input/ucidatasplits/test_df.csv').drop(columns=['Unnamed: 0'])\n",
    "    source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n",
    "\n",
    "    train_df['image'] = train_df['path'].map(lambda x: transform_train(Image.open(source+x[5:27]+'/'+x[-16:]).convert(\"RGB\")))\n",
    "    val_df['image'] = val_df['path'].map(lambda x: transform_val(Image.open(source+x[5:27]+'/'+x[-16:]).convert(\"RGB\")))\n",
    "    test_df['image'] = test_df['path'].map(lambda x: transform_val(Image.open(source+x[5:27]+'/'+x[-16:]).convert(\"RGB\")))\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:33:24.984288Z",
     "iopub.execute_input": "2024-10-20T13:33:24.984631Z",
     "iopub.status.idle": "2024-10-20T13:33:24.993109Z",
     "shell.execute_reply.started": "2024-10-20T13:33:24.984605Z",
     "shell.execute_reply": "2024-10-20T13:33:24.992228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\n\n# Define the MLP\nfrom torchvision.models import ResNet18_Weights\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\n\nclass ImageEffNet(nn.Module):\n    def __init__(self, output_size, num_classes):\n        super(ImageEffNet, self).__init__()\n        self.net = models.efficientnet_b0(weights='DEFAULT')\n        self.net.classifier[1] = nn.Linear(self.net.classifier[1].in_features, output_size)\n        self.fc = nn.Linear(output_size, num_classes)\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.fc(x)\n        return x\n    \n# class ImageResnet(nn.Module):\n#     def __init__(self,  output_size, num_classes):\n#         super(ImageResnet, self).__init__()\n#         self.net = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n#         # Modify the last layer to output 7 classes\n#          #2048\n#         num_features = self.net.fc.in_features\n#         self.net.fc = nn.Linear(num_features, output_size)\n#         self.fc = nn.Linear(output_size, num_classes)\n\n#     def forward(self, x):\n#         x = self.net(x)\n#         x = self.fc(x)\n#         return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-01T13:58:31.225714Z",
     "iopub.execute_input": "2025-05-01T13:58:31.226183Z",
     "iopub.status.idle": "2025-05-01T13:58:47.401316Z",
     "shell.execute_reply.started": "2025-05-01T13:58:31.226104Z",
     "shell.execute_reply": "2025-05-01T13:58:47.400231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-05-01 13:58:35.392272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-05-01 13:58:35.392447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-05-01 13:58:35.582355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "# we test the which layer weights are different",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def _get_model_Eff():\n    hidden_size = 128\n    output_size = 64\n    num_classes = 7\n    unet = ImageEffNet(output_size, num_classes)\n    return unet\nimport torch\n\n# Assuming model1 and model2 are the two EfficientNet-B0 models\ndef compare_models(model1, model2):\n    differences = []\n    \n    # Iterate over both models' named parameters\n    for (name1, param1), (name2, param2) in zip(model1.named_parameters(), model2.named_parameters()):\n        # Check if the parameter names are the same\n        if name1 != name2:\n            raise ValueError(f\"Layer mismatch: {name1} != {name2}\")\n        \n        # Compare the weights\n        if not torch.equal(param1, param2):\n            differences.append(name1)\n    \n    return differences\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-01T13:58:47.403338Z",
     "iopub.execute_input": "2025-05-01T13:58:47.404169Z",
     "iopub.status.idle": "2025-05-01T13:58:47.412042Z",
     "shell.execute_reply.started": "2025-05-01T13:58:47.404116Z",
     "shell.execute_reply": "2025-05-01T13:58:47.410825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "model2 = _get_model_Eff()  # Load or initialize the second EfficientNet-B0 model\nmodel_path= '/kaggle/input/retrain-model/retrain_eff_combine_model_tune1.pt'\nmodel2.load_state_dict(torch.load(model_path, map_location='cpu')) # ",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-01T13:59:10.319989Z",
     "iopub.execute_input": "2025-05-01T13:59:10.320357Z",
     "iopub.status.idle": "2025-05-01T13:59:11.071381Z",
     "shell.execute_reply.started": "2025-05-01T13:59:10.320324Z",
     "shell.execute_reply": "2025-05-01T13:59:11.070139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 112MB/s] \n",
     "output_type": "stream"
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "model2.net.classifier[1]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-01T13:59:28.284372Z",
     "iopub.execute_input": "2025-05-01T13:59:28.284792Z",
     "iopub.status.idle": "2025-05-01T13:59:28.292666Z",
     "shell.execute_reply.started": "2025-05-01T13:59:28.284753Z",
     "shell.execute_reply": "2025-05-01T13:59:28.291438Z"
    }
   },
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Linear(in_features=1280, out_features=64, bias=True)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# Example usage:\nmodel1 = ...  # Load or initialize the first EfficientNet-B0 model\n\nmodel1 = _get_model_Eff()\nmodel_path= '/kaggle/input/tunedmodel/eff_combine_model_tune.pt'\nmodel1.load_state_dict(torch.load(model_path, map_location='cpu')) # \n\nmodel2 = _get_model_Eff()  # Load or initialize the second EfficientNet-B0 model\nmodel_path= '/kaggle/input/retrain-model/retrain_eff_combine_model_tune1.pt'\nmodel2.load_state_dict(torch.load(model_path, map_location='cpu')) # \n\n# Get list of layers with different weights\ndifferent_layers = compare_models(model1, model2)\n\nif different_layers:\n    print(f\"Layers with different weights: {different_layers}\")\nelse:\n    print(\"All layers have the same weights.\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T05:40:27.769013Z",
     "iopub.execute_input": "2024-10-20T05:40:27.769414Z",
     "iopub.status.idle": "2024-10-20T05:40:29.480318Z",
     "shell.execute_reply.started": "2024-10-20T05:40:27.769381Z",
     "shell.execute_reply": "2024-10-20T05:40:29.478663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 99.6MB/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Layers with different weights: ['net.features.1.0.block.0.0.weight', 'net.features.1.0.block.0.1.weight', 'net.features.1.0.block.0.1.bias', 'net.features.1.0.block.1.fc1.weight', 'net.features.1.0.block.1.fc1.bias', 'net.features.1.0.block.1.fc2.weight', 'net.features.1.0.block.1.fc2.bias', 'net.features.1.0.block.2.0.weight', 'net.features.1.0.block.2.1.weight', 'net.features.1.0.block.2.1.bias']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# first we tested the untrained model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def _get_model_Eff():\n    hidden_size = 128\n    output_size = 64\n    num_classes = 7\n    unet = ImageEffNet(output_size, num_classes)\n    return unet\n\n\n# def _get_model_res():\n#     hidden_size = 128\n#     output_size = 64\n#     num_classes = 7\n#     unet = ImageResnet(output_size, num_classes)\n\n#     return unet",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:33:39.236510Z",
     "iopub.execute_input": "2024-10-20T13:33:39.236766Z",
     "iopub.status.idle": "2024-10-20T13:33:39.262516Z",
     "shell.execute_reply.started": "2024-10-20T13:33:39.236743Z",
     "shell.execute_reply": "2024-10-20T13:33:39.261497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "train_df, val_df, test_df = _get_df_data_()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:33:39.264244Z",
     "iopub.execute_input": "2024-10-20T13:33:39.264572Z",
     "iopub.status.idle": "2024-10-20T13:35:57.223429Z",
     "shell.execute_reply.started": "2024-10-20T13:33:39.264547Z",
     "shell.execute_reply": "2024-10-20T13:35:57.222454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)\nprint(train_df.shape[0] + val_df.shape[0]+ test_df.shape[0])\nprint(train_df.shape[0]/ (train_df.shape[0] + val_df.shape[0]+ test_df.shape[0]))\nprint( val_df.shape[0]/ (train_df.shape[0] + val_df.shape[0]+ test_df.shape[0]))\n\ndef retrain_perform():\n    BATCH_SIZE = 32\n    from torch.utils.data import DataLoader, WeightedRandomSampler\n    from collections import Counter\n    # Compute class weights for imbalanced dataset\n    class_counts = Counter([label for label in train_df['cell_type_idx']])\n    class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n    sample_weights = [class_weights[label] for label in train_df['cell_type_idx']]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n\n    image_source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n    train_data = dataMedicalImage(train_df, image_folder= image_source, transform=transform_train)\n    val_data = dataMedicalImage(val_df,image_folder= image_source,  transform=transform_val)\n    test_data = dataMedicalImage(test_df, image_folder= image_source, transform=transform_val)\n    \n    # Data loaders\n    trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=sampler)\n    valloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n    testloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    print(f\"Total number of batches in Train Loader: {len(trainloader)}\")\n    print(f\"Total number of batches in Val Loader: {len(valloader)}\")\n\n    model2 = _get_model_Eff()  # Load or initialize the second EfficientNet-B0 model\n    model_path= '/kaggle/input/retrain-model/retrain_eff_combine_model_tune1.pt'\n    model2.load_state_dict(torch.load(model_path)) #, map_location='cpu'\n    \n    test_model(model2, testloader)\nretrain_perform()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T05:53:46.621359Z",
     "iopub.execute_input": "2024-10-20T05:53:46.622246Z",
     "iopub.status.idle": "2024-10-20T05:53:49.683984Z",
     "shell.execute_reply.started": "2024-10-20T05:53:46.622214Z",
     "shell.execute_reply": "2024-10-20T05:53:49.683077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "(5229, 11)\n(1120, 11)\n(1121, 11)\n7470\n0.7\n0.1499330655957162\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 87.24%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.62      0.71      0.66        34\n         mel       0.84      0.88      0.86        49\n         bkl       0.70      0.71      0.70       109\n         bcc       0.56      0.45      0.50        11\n       akiec       0.95      0.93      0.94       811\n        vasc       0.61      0.62      0.61        92\n          df       0.65      1.00      0.79        15\n\n    accuracy                           0.87      1121\n   macro avg       0.70      0.76      0.72      1121\nweighted avg       0.88      0.87      0.87      1121\n\nConfusion Matrix:\n [[ 24   0   5   0   2   2   1]\n [  2  43   1   0   2   0   1]\n [  5   3  77   2  14   8   0]\n [  2   1   1   5   1   0   1]\n [  3   2  18   0 757  27   4]\n [  3   2   8   2  19  57   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "class dataMedicalImage(Dataset):\n    def __init__(self, dataframe, image_folder=None, transform=None):\n        self.dataframe = dataframe\n        self.folder = image_folder\n        self.transform = transform # already used on df-image creation process\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        image = self.dataframe.iloc[idx]['image']\n        label = self.dataframe.iloc[idx]['cell_type_idx']  # Assuming the label is in the second column\n        \n        return image, int(label)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:36:02.317783Z",
     "iopub.execute_input": "2024-10-20T13:36:02.318655Z",
     "iopub.status.idle": "2024-10-20T13:36:02.324586Z",
     "shell.execute_reply.started": "2024-10-20T13:36:02.318620Z",
     "shell.execute_reply": "2024-10-20T13:36:02.323643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef test_model(model, data_loader, class_names = lesion_type_dict.keys()):\n    model.eval()\n    model.to(device)\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    # Calculate accuracy\n    acc = 100 * sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\n    print('Current accuracy is: {:.2f}%'.format(acc))\n\n    # Generate classification report\n    report = classification_report(all_labels, all_predictions, target_names=class_names)\n    print(\"Classification Report:\\n\", report)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(all_labels, all_predictions)\n    print(\"Confusion Matrix:\\n\", cm)\n\n    # # Plot confusion matrix\n#     plt.figure(figsize=(10, 7))\n#     sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n#     plt.xlabel('Predicted')\n#     plt.ylabel('True')\n#     plt.title('Confusion Matrix')\n#     plt.show()\n\n    return acc\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Training function\ndef train_model(model, dataloader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, label in dataloader:\n            optimizer.zero_grad()\n            images = images.to(device)\n            label = label.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, label)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        epoch_loss = running_loss / len(dataloader)\n        print(f\"Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss}\")\n\n    return model\n\n\n# Function to evaluate the model\ndef eval_model(model, data_loader):\n    total, correct = 0, 0\n    model.eval()\n\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        acc = 100 * correct / total\n    print('current accuracy is ', acc)\n    return acc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:36:04.082677Z",
     "iopub.execute_input": "2024-10-20T13:36:04.083055Z",
     "iopub.status.idle": "2024-10-20T13:36:04.151514Z",
     "shell.execute_reply.started": "2024-10-20T13:36:04.083026Z",
     "shell.execute_reply": "2024-10-20T13:36:04.150605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# this is retrain this old model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def main_eff_retrain(model = None, name='test', r_laye= 1):\n    # Hyperparameters\n    num_epochs_mlp = 1\n    num_epochs_total = 10  # Total epochs including both MLP and U-Net\n    learning_rate = 0.001\n    BATCH_SIZE = 32\n\n    # Now you can train the model on your dataset\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n#   this is for tune only, 1 and 2, the follows is only tune \n#     optimizer = optim.Adam(model.net.features[r_laye].parameters(), lr=1e-4)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n#     optimizer = optim.Adam(\n#     list(model.net.features[1].parameters()) + list(model.net.features[2].parameters()), \n#     lr=1e-4)\n    \n    from torch.utils.data import DataLoader, WeightedRandomSampler\n    from collections import Counter\n    # Compute class weights for imbalanced dataset\n    class_counts = Counter([label for label in train_df['cell_type_idx']])\n    class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n    sample_weights = [class_weights[label] for label in train_df['cell_type_idx']]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n\n    image_source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n    train_data = dataMedicalImage(train_df, image_folder= image_source, transform=transform_train)\n    val_data = dataMedicalImage(val_df,image_folder= image_source,  transform=transform_val)\n    test_data = dataMedicalImage(test_df, image_folder= image_source, transform=transform_val)\n    \n    # Data loaders\n    trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=sampler)\n    valloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n    testloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    print(f\"Total number of batches in Train Loader: {len(trainloader)}\")\n    print(f\"Total number of batches in Val Loader: {len(valloader)}\")\n\n    best_model = None\n    accf = 0\n    test_model(model, testloader)\n    \n    for epoch in range(0, num_epochs_total, 1):\n        # Train MLP for 10 epochs\n        model = train_model(model, trainloader, criterion, optimizer, num_epochs_mlp)\n\n        acc = eval_model(model, valloader)\n        if acc > accf:\n            accf = acc\n            best_model = model\n            torch.save(best_model.state_dict(), 'retrain_eff_old_model_'+name+'.pt')\n    test_model(best_model, testloader)\n    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:44:09.459707Z",
     "iopub.execute_input": "2024-10-20T13:44:09.460563Z",
     "iopub.status.idle": "2024-10-20T13:44:09.473426Z",
     "shell.execute_reply.started": "2024-10-20T13:44:09.460528Z",
     "shell.execute_reply": "2024-10-20T13:44:09.472353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "def retrain_model_eff(name='', layid = 1):\n    model_eff = _get_model_Eff()\n    # retune the first one\n    model_path= '/kaggle/input/tunedmodel/eff_combine_model_tune.pt'\n    model_eff.load_state_dict(torch.load(model_path)) # , map_location='cpu'\n\n    # Freeze all parameters\n    for param in model_eff.net.parameters():\n        param.requires_grad = False\n\n    # Unfreeze only the parameters of the specific layer you want to tune\n    for param in model_eff.net.features[layid].parameters():\n        param.requires_grad = True\n    \n    main_eff_retrain(model = model_eff, name=('model_eff1'+name), r_laye= layid)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:36:17.090794Z",
     "iopub.execute_input": "2024-10-20T13:36:17.091173Z",
     "iopub.status.idle": "2024-10-20T13:36:17.098045Z",
     "shell.execute_reply.started": "2024-10-20T13:36:17.091146Z",
     "shell.execute_reply": "2024-10-20T13:36:17.096855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "def retrain_model_all(name='', layid = 1):\n    model_eff = _get_model_Eff()\n    # retune the first one\n    main_eff_retrain(model = model_eff, name=('model_eff1'+name), r_laye= layid)\n     \nimport time\nstart_time = time.time()\n\nretrain_model_all(name='', layid = 1)  \nprint(\"--- %s seconds ---\" % (time.time() - start_time))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-20T13:44:18.012454Z",
     "iopub.execute_input": "2024-10-20T13:44:18.012791Z",
     "iopub.status.idle": "2024-10-20T13:50:04.145128Z",
     "shell.execute_reply.started": "2024-10-20T13:44:18.012764Z",
     "shell.execute_reply": "2024-10-20T13:50:04.144232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 2.32%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.02      0.12      0.04        34\n         mel       0.00      0.00      0.00        49\n         bkl       0.00      0.00      0.00       109\n         bcc       0.00      0.00      0.00        11\n       akiec       0.00      0.00      0.00       811\n        vasc       0.09      0.11      0.10        92\n          df       0.01      0.80      0.03        15\n\n    accuracy                           0.02      1121\n   macro avg       0.02      0.15      0.02      1121\nweighted avg       0.01      0.02      0.01      1121\n\nConfusion Matrix:\n [[  4   0   0   0   0   2  28]\n [  5   0   0   0   0   0  44]\n [ 12   1   0   0   0   4  92]\n [  0   0   0   0   0   1  10]\n [134   2   1   0   0  91 583]\n [  8   0   0   0   0  10  74]\n [  3   0   0   0   0   0  12]]\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 0/0, Loss: 1.044206932368802\ncurrent accuracy is  72.94642857142857\nEpoch 0/0, Loss: 0.32953269161829135\ncurrent accuracy is  79.28571428571429\nEpoch 0/0, Loss: 0.20519960405895624\ncurrent accuracy is  79.46428571428571\nEpoch 0/0, Loss: 0.13409958372642172\ncurrent accuracy is  82.32142857142857\nEpoch 0/0, Loss: 0.10121991558045876\ncurrent accuracy is  83.39285714285714\nEpoch 0/0, Loss: 0.09114196706313367\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.06972247411826307\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.05906176533856679\ncurrent accuracy is  86.33928571428571\nEpoch 0/0, Loss: 0.04261758338898511\ncurrent accuracy is  86.33928571428571\nEpoch 0/0, Loss: 0.05412322893119758\ncurrent accuracy is  86.33928571428571\nCurrent accuracy is: 86.53%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.78      0.41      0.54        34\n         mel       0.80      0.80      0.80        49\n         bkl       0.74      0.64      0.69       109\n         bcc       0.70      0.64      0.67        11\n       akiec       0.93      0.95      0.94       811\n        vasc       0.52      0.60      0.56        92\n          df       0.69      0.73      0.71        15\n\n    accuracy                           0.87      1121\n   macro avg       0.74      0.68      0.70      1121\nweighted avg       0.87      0.87      0.86      1121\n\nConfusion Matrix:\n [[ 14   1  10   1   2   6   0]\n [  2  39   0   0   5   1   2]\n [  1   4  70   0  19  15   0]\n [  0   2   0   7   2   0   0]\n [  0   2   5   0 774  28   2]\n [  1   1  10   2  22  55   1]\n [  0   0   0   0   4   0  11]]\n--- 346.1270389556885 seconds ---\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "test time",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "october 20",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "CALCULATE TIME",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "this is results",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "87.24 + 86.17  + 86.26+85.99+87.15+86.98+86.80+86.62+86.44",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "change model, fixed all other layers and set free only on one layer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "model load and change",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# effnet retrain--",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def main_eff_retrain(model = None, name='test', r_laye= 1):\n    # Hyperparameters\n    num_epochs_mlp = 1\n    num_epochs_total = 10  # Total epochs including both MLP and U-Net\n    learning_rate = 0.001\n    BATCH_SIZE = 32\n\n    # Now you can train the model on your dataset\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n#   this is for tune only, 1 and 2, the follows is only tune \n    optimizer = optim.Adam(model.net.features[r_laye].parameters(), lr=1e-4)\n#     optimizer = optim.Adam(\n#     list(model.net.features[1].parameters()) + list(model.net.features[2].parameters()), \n#     lr=1e-4)\n    \n    from torch.utils.data import DataLoader, WeightedRandomSampler\n    from collections import Counter\n    # Compute class weights for imbalanced dataset\n    class_counts = Counter([label for label in train_df['cell_type_idx']])\n    class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n    sample_weights = [class_weights[label] for label in train_df['cell_type_idx']]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n\n    image_source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n    train_data = dataMedicalImage(train_df, image_folder= image_source, transform=transform_train)\n    val_data = dataMedicalImage(val_df,image_folder= image_source,  transform=transform_val)\n    test_data = dataMedicalImage(test_df, image_folder= image_source, transform=transform_val)\n    \n    # Data loaders\n    trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=sampler)\n    valloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n    testloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    print(f\"Total number of batches in Train Loader: {len(trainloader)}\")\n    print(f\"Total number of batches in Val Loader: {len(valloader)}\")\n\n    best_model = None\n    accf = 0\n    test_model(model, testloader)\n    \n    for epoch in range(0, num_epochs_total, 1):\n        # Train MLP for 10 epochs\n        model = train_model(model, trainloader, criterion, optimizer, num_epochs_mlp)\n\n        acc = eval_model(model, valloader)\n        if acc > accf:\n            accf = acc\n            best_model = model\n            torch.save(best_model.state_dict(), 'retrain_eff_model_'+name+'.pt')\n    test_model(best_model, testloader)\n    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T18:54:31.239671Z",
     "iopub.execute_input": "2024-09-27T18:54:31.240198Z",
     "iopub.status.idle": "2024-09-27T18:54:31.251856Z",
     "shell.execute_reply.started": "2024-09-27T18:54:31.240168Z",
     "shell.execute_reply": "2024-09-27T18:54:31.250887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "start retrain",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def retrain_model_eff(name='', layid = 1):\n    model_eff = _get_model_Eff()\n    # retune the first one\n    model_path= '/kaggle/input/retrainedmodelsep27/firsttrain_eff_model_f1v1.pt'\n    model_eff.load_state_dict(torch.load(model_path)) # , map_location='cpu'\n\n    # Freeze all parameters\n    for param in model_eff.net.parameters():\n        param.requires_grad = False\n\n    # Unfreeze only the parameters of the specific layer you want to tune\n    for param in model_eff.net.features[layid].parameters():\n        param.requires_grad = True\n    \n    main_eff_retrain(model = model_eff, name=('model_eff1'+name), r_laye= layid)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T18:54:40.117335Z",
     "iopub.execute_input": "2024-09-27T18:54:40.118172Z",
     "iopub.status.idle": "2024-09-27T18:54:40.126739Z",
     "shell.execute_reply.started": "2024-09-27T18:54:40.118129Z",
     "shell.execute_reply": "2024-09-27T18:54:40.125667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='v1', layid = 1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T12:42:30.394187Z",
     "iopub.execute_input": "2024-09-27T12:42:30.395077Z",
     "iopub.status.idle": "2024-09-27T12:45:42.663751Z",
     "shell.execute_reply.started": "2024-09-27T12:42:30.395031Z",
     "shell.execute_reply": "2024-09-27T12:45:42.662793Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.99%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.78      0.53      0.63        34\n         mel       0.83      0.80      0.81        49\n         bkl       0.60      0.80      0.68       109\n         bcc       0.67      0.55      0.60        11\n       akiec       0.95      0.92      0.94       811\n        vasc       0.54      0.54      0.54        92\n          df       0.93      0.93      0.93        15\n\n    accuracy                           0.86      1121\n   macro avg       0.76      0.72      0.73      1121\nweighted avg       0.87      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 18   0  11   0   4   1   0]\n [  2  39   4   1   3   0   0]\n [  2   3  87   0   8   9   0]\n [  0   2   1   6   1   1   0]\n [  0   1  27   1 750  32   0]\n [  1   2  16   0  22  50   1]\n [  0   0   0   1   0   0  14]]\nEpoch 0/0, Loss: 0.19048132279478922\ncurrent accuracy is  85.26785714285714\nEpoch 0/0, Loss: 0.181995705054028\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.18495557807022478\ncurrent accuracy is  85.80357142857143\nEpoch 0/0, Loss: 0.1960702133932855\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.1830150356130084\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.19093699392113017\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.18530852082980478\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.17162578696067013\ncurrent accuracy is  85.17857142857143\nEpoch 0/0, Loss: 0.17656731021749536\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.18257765111899593\ncurrent accuracy is  84.73214285714286\nCurrent accuracy is: 85.28%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.70      0.62      0.66        34\n         mel       0.80      0.80      0.80        49\n         bkl       0.62      0.74      0.68       109\n         bcc       0.71      0.45      0.56        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.50      0.63      0.56        92\n          df       0.79      1.00      0.88        15\n\n    accuracy                           0.85      1121\n   macro avg       0.73      0.74      0.72      1121\nweighted avg       0.87      0.85      0.86      1121\n\nConfusion Matrix:\n [[ 21   0   9   0   2   2   0]\n [  3  39   3   1   3   0   0]\n [  3   4  81   0  10  11   0]\n [  0   2   2   5   1   1   0]\n [  0   2  25   1 737  43   3]\n [  3   2  10   0  18  58   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='v1', layid = 2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T12:46:09.424665Z",
     "iopub.execute_input": "2024-09-27T12:46:09.425031Z",
     "iopub.status.idle": "2024-09-27T12:49:20.759292Z",
     "shell.execute_reply.started": "2024-09-27T12:46:09.425001Z",
     "shell.execute_reply": "2024-09-27T12:49:20.758410Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.99%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.78      0.53      0.63        34\n         mel       0.83      0.80      0.81        49\n         bkl       0.60      0.80      0.68       109\n         bcc       0.67      0.55      0.60        11\n       akiec       0.95      0.92      0.94       811\n        vasc       0.54      0.54      0.54        92\n          df       0.93      0.93      0.93        15\n\n    accuracy                           0.86      1121\n   macro avg       0.76      0.72      0.73      1121\nweighted avg       0.87      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 18   0  11   0   4   1   0]\n [  2  39   4   1   3   0   0]\n [  2   3  87   0   8   9   0]\n [  0   2   1   6   1   1   0]\n [  0   1  27   1 750  32   0]\n [  1   2  16   0  22  50   1]\n [  0   0   0   1   0   0  14]]\nEpoch 0/0, Loss: 0.18054940127881197\ncurrent accuracy is  85.53571428571429\nEpoch 0/0, Loss: 0.169950297550957\ncurrent accuracy is  85.625\nEpoch 0/0, Loss: 0.14895185513035736\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.13993065483958983\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.14562806000978482\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.13231268793730655\ncurrent accuracy is  85.98214285714286\nEpoch 0/0, Loss: 0.1363349253902348\ncurrent accuracy is  85.17857142857143\nEpoch 0/0, Loss: 0.1301438960462536\ncurrent accuracy is  85.17857142857143\nEpoch 0/0, Loss: 0.12751994553169735\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.12490725648453141\ncurrent accuracy is  85.35714285714286\nCurrent accuracy is: 86.17%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.67      0.53      0.59        34\n         mel       0.80      0.84      0.82        49\n         bkl       0.66      0.78      0.71       109\n         bcc       0.60      0.55      0.57        11\n       akiec       0.96      0.91      0.94       811\n        vasc       0.52      0.66      0.58        92\n          df       0.79      1.00      0.88        15\n\n    accuracy                           0.86      1121\n   macro avg       0.71      0.75      0.73      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 18   0  10   1   2   3   0]\n [  2  41   1   1   3   1   0]\n [  3   3  85   1   6  11   0]\n [  0   2   1   6   1   1   0]\n [  1   3  23   1 740  41   2]\n [  3   2   9   0  15  61   2]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print('this is layer 3')\nretrain_model_eff(name='v1', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T12:49:30.402038Z",
     "iopub.execute_input": "2024-09-27T12:49:30.402855Z",
     "iopub.status.idle": "2024-09-27T12:52:08.118472Z",
     "shell.execute_reply.started": "2024-09-27T12:49:30.402824Z",
     "shell.execute_reply": "2024-09-27T12:52:08.117586Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "this is layer 3\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.99%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.78      0.53      0.63        34\n         mel       0.83      0.80      0.81        49\n         bkl       0.60      0.80      0.68       109\n         bcc       0.67      0.55      0.60        11\n       akiec       0.95      0.92      0.94       811\n        vasc       0.54      0.54      0.54        92\n          df       0.93      0.93      0.93        15\n\n    accuracy                           0.86      1121\n   macro avg       0.76      0.72      0.73      1121\nweighted avg       0.87      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 18   0  11   0   4   1   0]\n [  2  39   4   1   3   0   0]\n [  2   3  87   0   8   9   0]\n [  0   2   1   6   1   1   0]\n [  0   1  27   1 750  32   0]\n [  1   2  16   0  22  50   1]\n [  0   0   0   1   0   0  14]]\nEpoch 0/0, Loss: 0.19846481746403363\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.16333340563834076\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.14677276799353067\ncurrent accuracy is  85.71428571428571\nEpoch 0/0, Loss: 0.1302307501730577\ncurrent accuracy is  85.80357142857143\nEpoch 0/0, Loss: 0.1102763245438748\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.11481094812099799\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.11421104614259447\ncurrent accuracy is  85.26785714285714\nEpoch 0/0, Loss: 0.10510712838447767\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.11463299416369055\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.09507120596301719\ncurrent accuracy is  85.0\nCurrent accuracy is: 86.08%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.77      0.50      0.61        34\n         mel       0.75      0.82      0.78        49\n         bkl       0.66      0.72      0.69       109\n         bcc       0.50      0.55      0.52        11\n       akiec       0.96      0.92      0.94       811\n        vasc       0.53      0.67      0.59        92\n          df       0.79      1.00      0.88        15\n\n    accuracy                           0.86      1121\n   macro avg       0.71      0.74      0.72      1121\nweighted avg       0.87      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 17   2   9   2   0   4   0]\n [  2  40   3   1   3   0   0]\n [  1   4  79   2  10  13   0]\n [  0   1   2   6   1   1   0]\n [  0   4  20   1 746  38   2]\n [  2   2   7   0  17  62   2]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "print('this is layer 4')\n",
    "retrain_model_eff(name='v1', layid = 4)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T12:55:15.834963Z",
     "iopub.execute_input": "2024-09-27T12:55:15.835623Z",
     "iopub.status.idle": "2024-09-27T12:57:32.612279Z",
     "shell.execute_reply.started": "2024-09-27T12:55:15.835571Z",
     "shell.execute_reply": "2024-09-27T12:57:32.611392Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "this is layer 3\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.99%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.78      0.53      0.63        34\n         mel       0.83      0.80      0.81        49\n         bkl       0.60      0.80      0.68       109\n         bcc       0.67      0.55      0.60        11\n       akiec       0.95      0.92      0.94       811\n        vasc       0.54      0.54      0.54        92\n          df       0.93      0.93      0.93        15\n\n    accuracy                           0.86      1121\n   macro avg       0.76      0.72      0.73      1121\nweighted avg       0.87      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 18   0  11   0   4   1   0]\n [  2  39   4   1   3   0   0]\n [  2   3  87   0   8   9   0]\n [  0   2   1   6   1   1   0]\n [  0   1  27   1 750  32   0]\n [  1   2  16   0  22  50   1]\n [  0   0   0   1   0   0  14]]\nEpoch 0/0, Loss: 0.1708666768703019\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.1334167865229907\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.11597217752302928\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.08600463002647568\ncurrent accuracy is  85.98214285714286\nEpoch 0/0, Loss: 0.08243576731964401\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.0804242716318496\ncurrent accuracy is  85.44642857142857\nEpoch 0/0, Loss: 0.08296977604239635\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.06783091837097323\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.0783280312714008\ncurrent accuracy is  85.80357142857143\nEpoch 0/0, Loss: 0.06559624178931345\ncurrent accuracy is  84.91071428571429\nCurrent accuracy is: 86.62%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.75      0.62      0.68        34\n         mel       0.84      0.86      0.85        49\n         bkl       0.67      0.72      0.70       109\n         bcc       0.71      0.45      0.56        11\n       akiec       0.96      0.92      0.94       811\n        vasc       0.51      0.67      0.58        92\n          df       0.82      0.93      0.87        15\n\n    accuracy                           0.87      1121\n   macro avg       0.75      0.74      0.74      1121\nweighted avg       0.88      0.87      0.87      1121\n\nConfusion Matrix:\n [[ 21   0   8   0   0   5   0]\n [  3  42   1   1   2   0   0]\n [  1   4  79   0  11  14   0]\n [  1   1   2   5   0   2   0]\n [  2   2  18   1 748  39   1]\n [  0   1  10   0  17  62   2]\n [  0   0   0   0   1   0  14]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": "train on the second",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-16T08:48:29.479957Z",
     "iopub.execute_input": "2024-08-16T08:48:29.480899Z",
     "iopub.status.idle": "2024-08-16T08:48:29.489237Z",
     "shell.execute_reply.started": "2024-08-16T08:48:29.480853Z",
     "shell.execute_reply": "2024-08-16T08:48:29.488271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "source": "main_eff definition",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# this is retrain reenet new models, ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def main_res_retrain(model=None, name='', layid = 1):\n    # Hyperparameters\n    num_epochs_mlp = 1\n    num_epochs_total = 10  # Total epochs including both MLP and U-Net\n    learning_rate = 0.001\n    BATCH_SIZE = 32\n    IMG_SIZE = 256\n\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.net.layer1.parameters(), lr=1e-4)\n\n    from torch.utils.data import DataLoader, WeightedRandomSampler\n    from collections import Counter\n    # Compute class weights for imbalanced dataset\n    class_counts = Counter([label for label in train_df['cell_type_idx']])\n    class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n    sample_weights = [class_weights[label] for label in train_df['cell_type_idx']]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n\n    image_source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n    train_data = dataMedicalImage(train_df, image_folder= image_source, transform=transform_train)\n    val_data = dataMedicalImage(val_df,image_folder= image_source,  transform=transform_val)\n    test_data = dataMedicalImage(test_df, image_folder= image_source, transform=transform_val)\n    \n    # Data loaders\n    trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=sampler)\n    valloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n    testloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    print(f\"Total number of batches in Train Loader: {len(trainloader)}\")\n    print(f\"Total number of batches in Val Loader: {len(valloader)}\")\n\n    best_model = None\n    accf = 0\n    print('first test original ')\n    test_model(model, testloader)\n    for epoch in range(0, num_epochs_total, 1):\n        # Train MLP for 10 epochs\n        model = train_model(model, trainloader, criterion, optimizer, num_epochs_mlp)\n\n        acc = eval_model(model, valloader)\n        if acc > accf:\n            accf = acc\n            best_model = model\n            torch.save(best_model.state_dict(), 'retrain_resnet_model_'+name+'.pt')\n\n    test_model(best_model, testloader)\n    \ndef retrain_model_res1(name='', layid = 1):\n    model_res = _get_model_res()\n    # retune the first one\n    model_path= '/kaggle/input/retrainedmodelsep27/firsttrain_resnet_model__f1v2.pt'\n    model_res.load_state_dict(torch.load(model_path)) # , map_location='cpu'\n\n    for param in model_res.net.parameters():\n        param.requires_grad = False\n    _nid = 'layer'+str(layid)\n    # Unfreeze only the parameters of the specific layer you want to tune\n    for param in model_res.net.layer1.parameters():\n        param.requires_grad = True\n    # Freeze all parameters\n    \n    main_res_retrain(model = model_res, name=('model_res1'+name), layid= layid)    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T20:45:22.278346Z",
     "iopub.execute_input": "2024-09-27T20:45:22.279229Z",
     "iopub.status.idle": "2024-09-27T20:45:22.293983Z",
     "shell.execute_reply.started": "2024-09-27T20:45:22.279194Z",
     "shell.execute_reply": "2024-09-27T20:45:22.292920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T20:29:30.249503Z",
     "iopub.execute_input": "2024-09-27T20:29:30.249875Z",
     "iopub.status.idle": "2024-09-27T20:29:30.256135Z",
     "shell.execute_reply.started": "2024-09-27T20:29:30.249846Z",
     "shell.execute_reply": "2024-09-27T20:29:30.255282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": "\nprint('this is v1 layer 1-1')\nretrain_model_res1(name='v1-L1-1', layid = 1)\nprint('this is v1 layer 1-2')\nretrain_model_res1(name='v1-L1-2', layid = 1)\nprint('this is v1 layer 1-3')\nretrain_model_res1(name='v1-L1-3', layid = 1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-27T20:04:34.691762Z",
     "iopub.execute_input": "2024-09-27T20:04:34.692112Z",
     "iopub.status.idle": "2024-09-27T20:09:39.112860Z",
     "shell.execute_reply.started": "2024-09-27T20:04:34.692084Z",
     "shell.execute_reply": "2024-09-27T20:09:39.111923Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "this is v1 layer 1-1\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 81.45%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.45      0.41      0.43        34\n         mel       0.71      0.65      0.68        49\n         bkl       0.54      0.67      0.60       109\n         bcc       0.83      0.45      0.59        11\n       akiec       0.91      0.91      0.91       811\n        vasc       0.44      0.38      0.41        92\n          df       0.92      0.80      0.86        15\n\n    accuracy                           0.81      1121\n   macro avg       0.69      0.61      0.64      1121\nweighted avg       0.82      0.81      0.81      1121\n\nConfusion Matrix:\n [[ 14   3   7   0   8   2   0]\n [  6  32   2   0   6   3   0]\n [  0   4  73   0  20  12   0]\n [  0   1   2   5   2   0   1]\n [  3   4  34   1 742  27   0]\n [  8   1  17   0  31  35   0]\n [  0   0   0   0   3   0  12]]\nEpoch 0/0, Loss: 0.8768145491073771\ncurrent accuracy is  78.57142857142857\nEpoch 0/0, Loss: 0.68278816242407\ncurrent accuracy is  77.85714285714286\nEpoch 0/0, Loss: 0.6045578874465896\ncurrent accuracy is  77.23214285714286\nEpoch 0/0, Loss: 0.4982129379925204\ncurrent accuracy is  76.25\nEpoch 0/0, Loss: 0.41353420195419616\ncurrent accuracy is  76.07142857142857\nEpoch 0/0, Loss: 0.38609926938647177\ncurrent accuracy is  75.80357142857143\nEpoch 0/0, Loss: 0.3394538963940449\ncurrent accuracy is  76.42857142857143\nEpoch 0/0, Loss: 0.3254095177597752\ncurrent accuracy is  76.60714285714286\nEpoch 0/0, Loss: 0.2911415483075671\ncurrent accuracy is  77.67857142857143\nEpoch 0/0, Loss: 0.2501160486873875\ncurrent accuracy is  77.41071428571429\nCurrent accuracy is: 77.61%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.33      0.65      0.44        34\n         mel       0.59      0.78      0.67        49\n         bkl       0.52      0.60      0.55       109\n         bcc       0.56      0.45      0.50        11\n       akiec       0.94      0.85      0.89       811\n        vasc       0.37      0.45      0.41        92\n          df       0.86      0.80      0.83        15\n\n    accuracy                           0.78      1121\n   macro avg       0.59      0.65      0.61      1121\nweighted avg       0.81      0.78      0.79      1121\n\nConfusion Matrix:\n [[ 22   5   6   0   1   0   0]\n [  9  38   0   0   2   0   0]\n [  9   5  65   2  14  14   0]\n [  0   3   1   5   2   0   0]\n [ 17   9  40   2 687  55   1]\n [ 10   4  14   0  22  41   1]\n [  0   0   0   0   3   0  12]]\nthis is v1 layer 1-2\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 81.45%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.45      0.41      0.43        34\n         mel       0.71      0.65      0.68        49\n         bkl       0.54      0.67      0.60       109\n         bcc       0.83      0.45      0.59        11\n       akiec       0.91      0.91      0.91       811\n        vasc       0.44      0.38      0.41        92\n          df       0.92      0.80      0.86        15\n\n    accuracy                           0.81      1121\n   macro avg       0.69      0.61      0.64      1121\nweighted avg       0.82      0.81      0.81      1121\n\nConfusion Matrix:\n [[ 14   3   7   0   8   2   0]\n [  6  32   2   0   6   3   0]\n [  0   4  73   0  20  12   0]\n [  0   1   2   5   2   0   1]\n [  3   4  34   1 742  27   0]\n [  8   1  17   0  31  35   0]\n [  0   0   0   0   3   0  12]]\nEpoch 0/0, Loss: 0.8548455308305054\ncurrent accuracy is  78.03571428571429\nEpoch 0/0, Loss: 0.7040684008289401\ncurrent accuracy is  77.14285714285714\nEpoch 0/0, Loss: 0.5553701008420165\ncurrent accuracy is  77.41071428571429\nEpoch 0/0, Loss: 0.4749424060715771\ncurrent accuracy is  77.32142857142857\nEpoch 0/0, Loss: 0.39542529005102994\ncurrent accuracy is  77.58928571428571\nEpoch 0/0, Loss: 0.37006642907948756\ncurrent accuracy is  76.96428571428571\nEpoch 0/0, Loss: 0.3310547397222097\ncurrent accuracy is  77.58928571428571\nEpoch 0/0, Loss: 0.3064523556655863\ncurrent accuracy is  77.32142857142857\nEpoch 0/0, Loss: 0.30369710783678583\ncurrent accuracy is  77.67857142857143\nEpoch 0/0, Loss: 0.26095958544713693\ncurrent accuracy is  77.85714285714286\nCurrent accuracy is: 77.70%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.37      0.65      0.47        34\n         mel       0.59      0.76      0.66        49\n         bkl       0.59      0.60      0.59       109\n         bcc       0.50      0.55      0.52        11\n       akiec       0.94      0.84      0.89       811\n        vasc       0.34      0.50      0.40        92\n          df       0.87      0.87      0.87        15\n\n    accuracy                           0.78      1121\n   macro avg       0.60      0.68      0.63      1121\nweighted avg       0.82      0.78      0.79      1121\n\nConfusion Matrix:\n [[ 22   4   4   0   3   1   0]\n [  9  37   0   0   2   1   0]\n [  7   3  65   3  13  18   0]\n [  0   2   1   6   2   0   0]\n [ 13  12  30   3 682  70   1]\n [  8   5  11   0  21  46   1]\n [  0   0   0   0   2   0  13]]\nthis is v1 layer 1-3\nTotal number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 81.45%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.45      0.41      0.43        34\n         mel       0.71      0.65      0.68        49\n         bkl       0.54      0.67      0.60       109\n         bcc       0.83      0.45      0.59        11\n       akiec       0.91      0.91      0.91       811\n        vasc       0.44      0.38      0.41        92\n          df       0.92      0.80      0.86        15\n\n    accuracy                           0.81      1121\n   macro avg       0.69      0.61      0.64      1121\nweighted avg       0.82      0.81      0.81      1121\n\nConfusion Matrix:\n [[ 14   3   7   0   8   2   0]\n [  6  32   2   0   6   3   0]\n [  0   4  73   0  20  12   0]\n [  0   1   2   5   2   0   1]\n [  3   4  34   1 742  27   0]\n [  8   1  17   0  31  35   0]\n [  0   0   0   0   3   0  12]]\nEpoch 0/0, Loss: 0.8855482176309679\ncurrent accuracy is  78.03571428571429\nEpoch 0/0, Loss: 0.6747158278779286\ncurrent accuracy is  77.32142857142857\nEpoch 0/0, Loss: 0.5671917884269866\ncurrent accuracy is  76.42857142857143\nEpoch 0/0, Loss: 0.4851376723679828\ncurrent accuracy is  77.05357142857143\nEpoch 0/0, Loss: 0.42699975513557836\ncurrent accuracy is  76.07142857142857\nEpoch 0/0, Loss: 0.36048591107402633\ncurrent accuracy is  76.96428571428571\nEpoch 0/0, Loss: 0.341668749250835\ncurrent accuracy is  75.80357142857143\nEpoch 0/0, Loss: 0.3208387796426328\ncurrent accuracy is  76.25\nEpoch 0/0, Loss: 0.27701802599448255\ncurrent accuracy is  77.76785714285714\nEpoch 0/0, Loss: 0.2657364572103067\ncurrent accuracy is  76.78571428571429\nCurrent accuracy is: 77.07%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.34      0.62      0.44        34\n         mel       0.55      0.80      0.65        49\n         bkl       0.53      0.62      0.57       109\n         bcc       0.50      0.55      0.52        11\n       akiec       0.94      0.84      0.89       811\n        vasc       0.35      0.43      0.39        92\n          df       0.86      0.80      0.83        15\n\n    accuracy                           0.77      1121\n   macro avg       0.58      0.66      0.61      1121\nweighted avg       0.81      0.77      0.79      1121\n\nConfusion Matrix:\n [[ 21   5   5   0   2   1   0]\n [  8  39   0   0   2   0   0]\n [  7   4  68   2  13  15   0]\n [  0   2   1   6   2   0   0]\n [ 17  15  39   3 678  58   1]\n [  8   6  15   0  22  40   1]\n [  0   0   0   1   2   0  12]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "main_res(model=model, name='layer3')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-16T12:55:09.859832Z",
     "iopub.execute_input": "2024-08-16T12:55:09.860546Z",
     "iopub.status.idle": "2024-08-16T12:58:48.284573Z",
     "shell.execute_reply.started": "2024-08-16T12:55:09.860515Z",
     "shell.execute_reply": "2024-08-16T12:58:48.283505Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 85.82%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.64      0.53      0.58        34\n         mel       0.76      0.76      0.76        49\n         bkl       0.69      0.69      0.69       109\n         bcc       1.00      0.36      0.53        11\n       akiec       0.91      0.96      0.94       811\n        vasc       0.58      0.41      0.48        92\n          df       0.85      0.73      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.63      0.68      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 18   4   6   0   4   2   0]\n [  3  37   3   0   5   0   1]\n [  5   2  75   0  18   9   0]\n [  0   1   1   4   5   0   0]\n [  1   1  13   0 779  17   0]\n [  1   2  10   0  40  38   1]\n [  0   2   0   0   2   0  11]]\nEpoch 0/0, Loss: 0.20393525351338634\ncurrent accuracy is  81.96428571428571\nEpoch 0/0, Loss: 0.10251373455791575\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.06544169762012798\ncurrent accuracy is  82.94642857142857\nEpoch 0/0, Loss: 0.05018448428727868\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.035654407585176025\ncurrent accuracy is  85.17857142857143\nEpoch 0/0, Loss: 0.03641699224415167\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.02929881636746137\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.025471132268515845\ncurrent accuracy is  85.44642857142857\nEpoch 0/0, Loss: 0.020745139341892267\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.020628356777994735\ncurrent accuracy is  85.35714285714286\nCurrent accuracy is: 86.17%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.59      0.38      0.46        34\n         mel       0.81      0.59      0.68        49\n         bkl       0.76      0.67      0.71       109\n         bcc       0.75      0.27      0.40        11\n       akiec       0.90      0.98      0.93       811\n        vasc       0.66      0.47      0.55        92\n          df       0.93      0.93      0.93        15\n\n    accuracy                           0.86      1121\n   macro avg       0.77      0.61      0.67      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 13   3   5   1   9   3   0]\n [  3  29   2   0  13   1   1]\n [  4   1  73   0  22   9   0]\n [  1   0   2   3   5   0   0]\n [  0   1  10   0 791   9   0]\n [  1   2   4   0  42  43   0]\n [  0   0   0   0   1   0  14]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": "main_res(model=model, name='layer2')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-16T13:01:28.317315Z",
     "iopub.execute_input": "2024-08-16T13:01:28.318186Z",
     "iopub.status.idle": "2024-08-16T13:03:40.752593Z",
     "shell.execute_reply.started": "2024-08-16T13:01:28.318151Z",
     "shell.execute_reply": "2024-08-16T13:03:40.751568Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 85.82%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.64      0.53      0.58        34\n         mel       0.76      0.76      0.76        49\n         bkl       0.69      0.69      0.69       109\n         bcc       1.00      0.36      0.53        11\n       akiec       0.91      0.96      0.94       811\n        vasc       0.58      0.41      0.48        92\n          df       0.85      0.73      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.63      0.68      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 18   4   6   0   4   2   0]\n [  3  37   3   0   5   0   1]\n [  5   2  75   0  18   9   0]\n [  0   1   1   4   5   0   0]\n [  1   1  13   0 779  17   0]\n [  1   2  10   0  40  38   1]\n [  0   2   0   0   2   0  11]]\nEpoch 0/0, Loss: 0.24255221720966624\ncurrent accuracy is  81.96428571428571\nEpoch 0/0, Loss: 0.12878705983663477\ncurrent accuracy is  82.76785714285714\nEpoch 0/0, Loss: 0.09131931007567157\ncurrent accuracy is  82.05357142857143\nEpoch 0/0, Loss: 0.07957418364041098\ncurrent accuracy is  82.85714285714286\nEpoch 0/0, Loss: 0.05794037585487453\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.05708644933226269\ncurrent accuracy is  84.01785714285714\nEpoch 0/0, Loss: 0.049830125451723974\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.042921736535457215\ncurrent accuracy is  83.39285714285714\nEpoch 0/0, Loss: 0.04074266431966751\ncurrent accuracy is  84.19642857142857\nEpoch 0/0, Loss: 0.03941918510330341\ncurrent accuracy is  84.91071428571429\nCurrent accuracy is: 85.73%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.51      0.74      0.60        34\n         mel       0.81      0.71      0.76        49\n         bkl       0.77      0.66      0.71       109\n         bcc       0.75      0.55      0.63        11\n       akiec       0.91      0.96      0.93       811\n        vasc       0.56      0.37      0.44        92\n          df       0.81      0.87      0.84        15\n\n    accuracy                           0.86      1121\n   macro avg       0.73      0.69      0.70      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 25   1   3   1   3   1   0]\n [  6  35   0   1   6   1   0]\n [  9   0  72   0  20   8   0]\n [  1   0   1   6   3   0   0]\n [  4   2  11   0 776  17   1]\n [  4   3   6   0  43  34   2]\n [  0   2   0   0   0   0  13]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "main_res(model=model, name='1and2')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-16T13:19:49.926110Z",
     "iopub.execute_input": "2024-08-16T13:19:49.926454Z",
     "iopub.status.idle": "2024-08-16T13:22:43.837066Z",
     "shell.execute_reply.started": "2024-08-16T13:19:49.926427Z",
     "shell.execute_reply": "2024-08-16T13:22:43.836028Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 85.82%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.64      0.53      0.58        34\n         mel       0.76      0.76      0.76        49\n         bkl       0.69      0.69      0.69       109\n         bcc       1.00      0.36      0.53        11\n       akiec       0.91      0.96      0.94       811\n        vasc       0.58      0.41      0.48        92\n          df       0.85      0.73      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.63      0.68      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 18   4   6   0   4   2   0]\n [  3  37   3   0   5   0   1]\n [  5   2  75   0  18   9   0]\n [  0   1   1   4   5   0   0]\n [  1   1  13   0 779  17   0]\n [  1   2  10   0  40  38   1]\n [  0   2   0   0   2   0  11]]\nEpoch 0/0, Loss: 0.2264026525407666\ncurrent accuracy is  80.98214285714286\nEpoch 0/0, Loss: 0.13033125671081064\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.0854362446769345\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.0768399083441714\ncurrent accuracy is  84.19642857142857\nEpoch 0/0, Loss: 0.056008025231521305\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.04846433977164873\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.046120446851113585\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.04543484545812556\ncurrent accuracy is  83.92857142857143\nEpoch 0/0, Loss: 0.041269350243255315\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.03694470733313299\ncurrent accuracy is  84.73214285714286\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.57      0.76      0.65        34\n         mel       0.86      0.63      0.73        49\n         bkl       0.66      0.72      0.69       109\n         bcc       0.75      0.55      0.63        11\n       akiec       0.93      0.94      0.94       811\n        vasc       0.58      0.47      0.52        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.74      0.72      0.72      1121\nweighted avg       0.86      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 26   1   5   0   1   1   0]\n [  5  31   5   0   7   1   0]\n [  7   1  78   2  15   6   0]\n [  1   0   1   6   3   0   0]\n [  4   1  18   0 764  23   1]\n [  3   2  11   0  31  43   2]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": "main_res(model=model, name='layer4')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-16T13:24:18.604541Z",
     "iopub.execute_input": "2024-08-16T13:24:18.605286Z",
     "iopub.status.idle": "2024-08-16T13:26:09.156557Z",
     "shell.execute_reply.started": "2024-08-16T13:24:18.605249Z",
     "shell.execute_reply": "2024-08-16T13:26:09.155589Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nfirst test original \nCurrent accuracy is: 85.82%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.64      0.53      0.58        34\n         mel       0.76      0.76      0.76        49\n         bkl       0.69      0.69      0.69       109\n         bcc       1.00      0.36      0.53        11\n       akiec       0.91      0.96      0.94       811\n        vasc       0.58      0.41      0.48        92\n          df       0.85      0.73      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.63      0.68      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 18   4   6   0   4   2   0]\n [  3  37   3   0   5   0   1]\n [  5   2  75   0  18   9   0]\n [  0   1   1   4   5   0   0]\n [  1   1  13   0 779  17   0]\n [  1   2  10   0  40  38   1]\n [  0   2   0   0   2   0  11]]\nEpoch 0/0, Loss: 0.20914420890962568\ncurrent accuracy is  83.03571428571429\nEpoch 0/0, Loss: 0.09174199153033154\ncurrent accuracy is  83.83928571428571\nEpoch 0/0, Loss: 0.050870264815053017\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.042309288713490455\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.02719708635165106\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.02754589630200005\ncurrent accuracy is  83.66071428571429\nEpoch 0/0, Loss: 0.01524151615917728\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.018585889373065496\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.014506082682268375\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.018767490289486353\ncurrent accuracy is  85.53571428571429\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.62      0.44      0.52        34\n         mel       0.77      0.61      0.68        49\n         bkl       0.68      0.71      0.69       109\n         bcc       0.50      0.36      0.42        11\n       akiec       0.92      0.96      0.94       811\n        vasc       0.56      0.48      0.52        92\n          df       0.88      0.93      0.90        15\n\n    accuracy                           0.86      1121\n   macro avg       0.71      0.64      0.67      1121\nweighted avg       0.85      0.86      0.85      1121\n\nConfusion Matrix:\n [[ 15   4   8   2   4   1   0]\n [  3  30   5   0   8   2   1]\n [  5   0  77   1  16  10   0]\n [  0   1   1   4   4   1   0]\n [  1   1  10   0 779  20   0]\n [  0   2  12   1  32  44   1]\n [  0   1   0   0   0   0  14]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": "check the feature size",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# model analysis by weightwatcher",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "[weightwatcher tool](https://github.com/CalculatedContent/WeightWatcher)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "pip install weightwatcher",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:44:19.059322Z",
     "iopub.execute_input": "2024-10-04T08:44:19.059699Z",
     "iopub.status.idle": "2024-10-04T08:44:34.884972Z",
     "shell.execute_reply.started": "2024-10-04T08:44:19.059668Z",
     "shell.execute_reply": "2024-10-04T08:44:34.883826Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting weightwatcher\n  Downloading weightwatcher-0.7.5.5-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (2.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (3.7.5)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (0.1.6)\nCollecting powerlaw (from weightwatcher)\n  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (1.2.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (0.4.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from weightwatcher) (4.66.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->weightwatcher) (2.9.0.post0)\nRequirement already satisfied: traitlets in /opt/conda/lib/python3.10/site-packages (from matplotlib-inline->weightwatcher) (5.9.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->weightwatcher) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->weightwatcher) (2023.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from powerlaw->weightwatcher) (1.11.4)\nRequirement already satisfied: mpmath in /opt/conda/lib/python3.10/site-packages (from powerlaw->weightwatcher) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->weightwatcher) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->weightwatcher) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->weightwatcher) (1.16.0)\nDownloading weightwatcher-0.7.5.5-py3-none-any.whl (81 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.9/81.9 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hDownloading powerlaw-1.5-py3-none-any.whl (24 kB)\nInstalling collected packages: powerlaw, weightwatcher\nSuccessfully installed powerlaw-1.5 weightwatcher-0.7.5.5\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\nimport weightwatcher as ww\nimport torchvision.models as models\n\n\n# Define the MLP\nfrom torchvision.models import ResNet18_Weights\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\n\nclass ImageEffNet(nn.Module):\n    def __init__(self, output_size, num_classes):\n        super(ImageEffNet, self).__init__()\n        self.net = models.efficientnet_b0(weights='DEFAULT')\n        self.net.classifier[1] = nn.Linear(self.net.classifier[1].in_features, output_size)\n        self.fc = nn.Linear(output_size, num_classes)\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.fc(x)\n        return x\n    \nclass ImageResnet(nn.Module):\n    def __init__(self,  output_size, num_classes):\n        super(ImageResnet, self).__init__()\n        self.net = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        # Modify the last layer to output 7 classes\n         #2048\n        num_features = self.net.fc.in_features\n        self.net.fc = nn.Linear(num_features, output_size)\n        self.fc = nn.Linear(output_size, num_classes)\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.fc(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:44:39.567034Z",
     "iopub.execute_input": "2024-10-04T08:44:39.568004Z",
     "iopub.status.idle": "2024-10-04T08:45:00.818234Z",
     "shell.execute_reply.started": "2024-10-04T08:44:39.567966Z",
     "shell.execute_reply": "2024-10-04T08:45:00.817366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2024-10-04 08:44:50.011760: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-04 08:44:50.011950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-04 08:44:50.192875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "def _get_model_Eff():\n    hidden_size = 128\n    output_size = 64\n    num_classes = 7\n    unet = ImageEffNet(output_size, num_classes)\n    # get the fine tune one-time model to see which layer is well-tuned or not\n    model_path= '/kaggle/input/tunedmodel/eff_combine_model_tune.pt'\n    unet.load_state_dict(torch.load(model_path, map_location='cpu')) # , map_location='cpu'\n    return unet\n\n\ndef _get_model_res():\n    hidden_size = 128\n    output_size = 64\n    num_classes = 7\n    model = ImageResnet(output_size, num_classes)\n    #  get the fine tune one-time model to see which layer is well-tuned or not\n    model_path= '/kaggle/input/tunedmodel/res_combine_model_tune.pt'\n    model.load_state_dict(torch.load(model_path, map_location='cpu')) # \n    return model",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:45:02.116630Z",
     "iopub.execute_input": "2024-10-04T08:45:02.117652Z",
     "iopub.status.idle": "2024-10-04T08:45:02.123865Z",
     "shell.execute_reply.started": "2024-10-04T08:45:02.117616Z",
     "shell.execute_reply": "2024-10-04T08:45:02.122838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def eff_analyze():\n    model_eff = _get_model_Eff()\n    watcher = ww.WeightWatcher(model=model_eff)\n#     details = watcher.analyze()\n    details = watcher.analyze(ww2x=True, randomize=True, mp_fit=True, min_evals=10)\n    summary = watcher.get_summary(details)\n    return summary, details\n\ndef res_analyze():\n    model_res = _get_model_res()\n    watcher = ww.WeightWatcher(model=model_res)\n#     details = watcher.analyze()\n    details = watcher.analyze(ww2x=True, randomize=True, mp_fit=True, min_evals=10)\n    summary = watcher.get_summary(details)\n    return summary,details",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:45:04.644976Z",
     "iopub.execute_input": "2024-10-04T08:45:04.645688Z",
     "iopub.status.idle": "2024-10-04T08:45:04.654313Z",
     "shell.execute_reply.started": "2024-10-04T08:45:04.645644Z",
     "shell.execute_reply": "2024-10-04T08:45:04.653395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T12:39:14.845099Z",
     "iopub.execute_input": "2024-10-03T12:39:14.845459Z",
     "iopub.status.idle": "2024-10-03T12:39:16.239986Z",
     "shell.execute_reply.started": "2024-10-03T12:39:14.845420Z",
     "shell.execute_reply": "2024-10-03T12:39:16.238998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "sum_eff, del_eff = eff_analyze()\ndel_eff.to_csv('details_eff.csv')\nsum_res, del_res = res_analyze()\ndel_res.to_csv('details_res.csv')\ndel_res_1 = del_res[['longname', 'alpha', 'rank_loss', 'status', 'warning']]\ndel_eff_1 = del_eff[['longname', 'alpha', 'rank_loss', 'status', 'warning']]\ndel_eff_1.to_csv('eff_selected_detail.csv')\ndel_res_1.to_csv('res_selected_detail.csv')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:45:06.903792Z",
     "iopub.execute_input": "2024-10-04T08:45:06.904717Z",
     "iopub.status.idle": "2024-10-04T08:52:42.358331Z",
     "shell.execute_reply.started": "2024-10-04T08:45:06.904683Z",
     "shell.execute_reply": "2024-10-04T08:52:42.357510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 108MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 142MB/s] \n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (6, 4),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\nplt.rcParams.update(params)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:53:07.696655Z",
     "iopub.execute_input": "2024-10-04T08:53:07.697334Z",
     "iopub.status.idle": "2024-10-04T08:53:07.702283Z",
     "shell.execute_reply.started": "2024-10-04T08:53:07.697300Z",
     "shell.execute_reply": "2024-10-04T08:53:07.701245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "del_eff.alpha.plot.hist(bins=4, title='Efficientnet ($M_{s2}$)')\nplt.xlabel(\"weightwatcher layer quality metric alpha\")\nplt.axvline(x=2.0, color='red')\nplt.axvline(x=6.0, color='orange')\nplt.savefig('effnet.alpha.png',bbox_inches='tight', dpi=150)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:54:58.411349Z",
     "iopub.execute_input": "2024-10-04T08:54:58.411778Z",
     "iopub.status.idle": "2024-10-04T08:54:58.961516Z",
     "shell.execute_reply.started": "2024-10-04T08:54:58.411746Z",
     "shell.execute_reply": "2024-10-04T08:54:58.960727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "sum_res, del_res = res_analyze()\ndel_res.to_csv('details_res.csv')\nsum_res",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T12:41:25.646272Z",
     "iopub.execute_input": "2024-10-03T12:41:25.646648Z",
     "iopub.status.idle": "2024-10-03T12:41:30.404571Z",
     "shell.execute_reply.started": "2024-10-03T12:41:25.646617Z",
     "shell.execute_reply": "2024-10-03T12:41:30.403635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 152MB/s] \n",
     "output_type": "stream"
    },
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'log_norm': 1.9124785038192829,\n 'alpha': 3.469104554459105,\n 'alpha_weighted': 0.4716515893343965,\n 'log_alpha_norm': 1.1314091215888757,\n 'log_spectral_norm': 0.17500931433254005,\n 'stable_rank': 77.82423289564538}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "del_res.alpha.plot.hist(bins=4, title='Resnet18 ($M_{s2}$)')\nplt.xlabel(\"weightwatcher layer quality metric alpha\")\nplt.axvline(x=2.0, color='red')\nplt.axvline(x=6.0, color='orange')\nplt.savefig('resnet.alpha.png',bbox_inches='tight', dpi=150)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:55:28.107952Z",
     "iopub.execute_input": "2024-10-04T08:55:28.108617Z",
     "iopub.status.idle": "2024-10-04T08:55:28.672497Z",
     "shell.execute_reply.started": "2024-10-04T08:55:28.108582Z",
     "shell.execute_reply": "2024-10-04T08:55:28.671443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T13:06:28.171756Z",
     "iopub.execute_input": "2024-10-03T13:06:28.174876Z",
     "iopub.status.idle": "2024-10-03T13:06:28.180306Z",
     "shell.execute_reply.started": "2024-10-03T13:06:28.174842Z",
     "shell.execute_reply": "2024-10-03T13:06:28.179416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": "del_res_1 = del_res[['longname', 'alpha', 'rank_loss', 'status', 'warning']]\ndel_eff_1 = del_eff[['longname', 'alpha', 'rank_loss', 'status', 'warning']]\ndel_eff_1.to_csv('eff_selected_detail.csv')\ndel_res_1.to_csv('res_selected_detail.csv')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T13:07:23.974603Z",
     "iopub.execute_input": "2024-10-03T13:07:23.975330Z",
     "iopub.status.idle": "2024-10-03T13:07:23.984658Z",
     "shell.execute_reply.started": "2024-10-03T13:07:23.975288Z",
     "shell.execute_reply": "2024-10-03T13:07:23.983673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "# this is to use weightwatcher to tuned models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def retrain_model_eff1(name='', layid = 1):\n    model_eff = _get_model_Eff()\n    model_1 = _get_model_Eff()\n    # retune the first one\n    model_path= '/kaggle/input/tunedmodel/eff_combine_model_tune.pt'\n    model_eff.load_state_dict(torch.load(model_path)) # , map_location='cpu'\n    model_eff = change_parameters(model_1, model_eff)\n       \n    main_eff_retrain(model = model_eff, name=('model_eff1'+name), r_laye= layid)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:18:03.630790Z",
     "iopub.execute_input": "2024-10-04T13:18:03.631374Z",
     "iopub.status.idle": "2024-10-04T13:18:03.636600Z",
     "shell.execute_reply.started": "2024-10-04T13:18:03.631342Z",
     "shell.execute_reply": "2024-10-04T13:18:03.635686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "\ndef retrain_model_eff(name='', layid = 1):\n    model1 =  _get_model_Eff()\n    # Load model2 with custom weights\n    model2 =  _get_model_Eff()  # Initialize with no pre-trained weights\n    model2.load_state_dict(torch.load('/kaggle/input/tunedmodel/eff_combine_model_tune.pt'))\n\n    # Freeze all parameters\n    for param in model2.net.parameters():\n        param.requires_grad = False\n        \n    model2 = change_parameters(model1, model2)\n    \n    # Collect parameters to tune (those from under-trained layers)\n    params_to_tune = []\n\n    for layer_name, status in layer_replacement_info.items():\n            # Helper function to get submodule by name\n        def get_submodule_by_name(model, name):\n            for n, m in model.named_modules():\n                if n == name:\n                    return m\n            return None\n        # Get the submodule from model2\n        layer_model2 = get_submodule_by_name(model2, layer_name) \n        # Collect the parameters from the layer\n        if layer_model2 is not None:\n            params_to_tune += list(layer_model2.parameters())\n            \n        layer_model2 = get_submodule_by_name(model2, layer_name)\n        for param in layer_model2.parameters():\n             param.requires_grad = True\n#     print(params_to_tune)\n    main_eff_retrain(model = model2, name=('model_eff1'+name), params_to_tune=params_to_tune)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:18:01.840012Z",
     "iopub.execute_input": "2024-10-04T13:18:01.840817Z",
     "iopub.status.idle": "2024-10-04T13:18:01.850969Z",
     "shell.execute_reply.started": "2024-10-04T13:18:01.840778Z",
     "shell.execute_reply": "2024-10-04T13:18:01.849756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher', layid = 3)",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.19651673089076832\ncurrent accuracy is  83.57142857142857\nEpoch 0/0, Loss: 0.16785594467755136\ncurrent accuracy is  83.39285714285714\nEpoch 0/0, Loss: 0.15870806052372222\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.1452503855135746\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.12963390207236133\ncurrent accuracy is  85.625\nEpoch 0/0, Loss: 0.1284147742738175\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.11731044592324463\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.11756154031083896\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.10778505478917462\ncurrent accuracy is  85.625\nEpoch 0/0, Loss: 0.10134747157032352\ncurrent accuracy is  85.53571428571429\nCurrent accuracy is: 86.08%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.68      0.68      0.68        34\n         mel       0.81      0.86      0.83        49\n         bkl       0.71      0.75      0.73       109\n         bcc       0.67      0.36      0.47        11\n       akiec       0.96      0.91      0.94       811\n        vasc       0.48      0.65      0.56        92\n          df       0.75      1.00      0.86        15\n\n    accuracy                           0.86      1121\n   macro avg       0.72      0.74      0.72      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 23   0   6   1   2   2   0]\n [  2  42   1   0   3   0   1]\n [  4   5  82   0   5  13   0]\n [  0   2   2   4   2   0   1]\n [  2   2  17   0 739  49   2]\n [  3   1   8   1  18  60   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher1', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T10:22:51.773069Z",
     "iopub.execute_input": "2024-10-04T10:22:51.773484Z",
     "iopub.status.idle": "2024-10-04T10:26:44.139916Z",
     "shell.execute_reply.started": "2024-10-04T10:22:51.773452Z",
     "shell.execute_reply": "2024-10-04T10:26:44.138931Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.18552498015143523\ncurrent accuracy is  83.21428571428571\nEpoch 0/0, Loss: 0.17384744472460958\ncurrent accuracy is  83.48214285714286\nEpoch 0/0, Loss: 0.15757615541721262\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.1334948300883719\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.12646480896169457\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.12803722297909056\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.10702438969392239\ncurrent accuracy is  83.83928571428571\nEpoch 0/0, Loss: 0.11287661722475072\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.1033497721967628\ncurrent accuracy is  85.44642857142857\nEpoch 0/0, Loss: 0.10298348419515915\ncurrent accuracy is  84.46428571428571\nCurrent accuracy is: 85.37%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.67      0.65      0.66        34\n         mel       0.83      0.88      0.85        49\n         bkl       0.68      0.72      0.70       109\n         bcc       0.62      0.45      0.53        11\n       akiec       0.97      0.90      0.93       811\n        vasc       0.46      0.72      0.56        92\n          df       0.68      1.00      0.81        15\n\n    accuracy                           0.85      1121\n   macro avg       0.70      0.76      0.72      1121\nweighted avg       0.88      0.85      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   4   1   1   5   1]\n [  2  43   1   0   2   0   1]\n [  5   3  79   0   5  17   0]\n [  1   2   2   5   0   0   1]\n [  1   3  23   0 727  54   3]\n [  2   1   7   2  13  66   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher2', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T10:27:39.406508Z",
     "iopub.execute_input": "2024-10-04T10:27:39.407334Z",
     "iopub.status.idle": "2024-10-04T10:31:31.808192Z",
     "shell.execute_reply.started": "2024-10-04T10:27:39.407295Z",
     "shell.execute_reply": "2024-10-04T10:31:31.807271Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.19484343380871705\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.15361126447550771\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.14818861659207358\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.1382406855161052\ncurrent accuracy is  84.19642857142857\nEpoch 0/0, Loss: 0.14340175986971435\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.12565679092914228\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.1194388572433282\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.1144572880016867\ncurrent accuracy is  85.26785714285714\nEpoch 0/0, Loss: 0.10153752632393706\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.10125025243657391\ncurrent accuracy is  84.91071428571429\nCurrent accuracy is: 85.01%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.59      0.65      0.62        34\n         mel       0.85      0.84      0.85        49\n         bkl       0.69      0.72      0.70       109\n         bcc       0.71      0.45      0.56        11\n       akiec       0.97      0.89      0.93       811\n        vasc       0.44      0.73      0.55        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.85      1121\n   macro avg       0.73      0.75      0.73      1121\nweighted avg       0.88      0.85      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   1   0   5   0]\n [  3  41   2   0   2   0   1]\n [  5   2  78   0   6  18   0]\n [  1   1   2   5   1   1   0]\n [  2   3  18   0 725  62   1]\n [  4   1   7   1  11  67   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher3', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T10:32:19.613789Z",
     "iopub.execute_input": "2024-10-04T10:32:19.614733Z",
     "iopub.status.idle": "2024-10-04T10:36:12.223918Z",
     "shell.execute_reply.started": "2024-10-04T10:32:19.614696Z",
     "shell.execute_reply": "2024-10-04T10:36:12.222918Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.20896803477506448\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.16606058642586194\ncurrent accuracy is  84.19642857142857\nEpoch 0/0, Loss: 0.14459813743966018\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.13440371126436243\ncurrent accuracy is  84.19642857142857\nEpoch 0/0, Loss: 0.12187437026011871\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.12546159864244302\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.11939513836646588\ncurrent accuracy is  85.44642857142857\nEpoch 0/0, Loss: 0.1225299486754144\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.10552908007123667\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.0961350538858735\ncurrent accuracy is  85.625\nCurrent accuracy is: 86.44%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.71      0.65      0.68        34\n         mel       0.91      0.84      0.87        49\n         bkl       0.71      0.73      0.72       109\n         bcc       0.58      0.64      0.61        11\n       akiec       0.96      0.92      0.94       811\n        vasc       0.49      0.66      0.56        92\n          df       0.65      1.00      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.72      0.78      0.74      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 22   0   4   1   1   5   1]\n [  2  41   0   0   4   1   1]\n [  3   1  80   0  10  15   0]\n [  0   1   1   7   1   0   1]\n [  1   1  19   0 743  43   4]\n [  3   1   8   4  14  61   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher4', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:18:36.315429Z",
     "iopub.execute_input": "2024-10-04T13:18:36.316056Z",
     "iopub.status.idle": "2024-10-04T13:22:24.299276Z",
     "shell.execute_reply.started": "2024-10-04T13:18:36.316026Z",
     "shell.execute_reply": "2024-10-04T13:22:24.298092Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 125MB/s] \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.20881807924498144\ncurrent accuracy is  82.85714285714286\nEpoch 0/0, Loss: 0.1765629678010577\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.14880668938659677\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.13147769365232528\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.1411253282652668\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.13142136602503499\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.10279938170868086\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.11555852564960355\ncurrent accuracy is  85.53571428571429\nEpoch 0/0, Loss: 0.09901706350785566\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.09699075327754565\ncurrent accuracy is  84.55357142857143\nCurrent accuracy is: 85.37%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.63      0.71      0.67        34\n         mel       0.91      0.86      0.88        49\n         bkl       0.72      0.74      0.73       109\n         bcc       0.70      0.64      0.67        11\n       akiec       0.97      0.89      0.93       811\n        vasc       0.45      0.68      0.55        92\n          df       0.60      1.00      0.75        15\n\n    accuracy                           0.85      1121\n   macro avg       0.71      0.79      0.74      1121\nweighted avg       0.88      0.85      0.86      1121\n\nConfusion Matrix:\n [[ 24   0   4   1   0   4   1]\n [  3  42   1   0   1   1   1]\n [  6   1  81   0   8  13   0]\n [  1   1   0   7   1   0   1]\n [  2   1  20   0 725  58   5]\n [  2   1   6   2  16  63   2]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher5', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:23:06.501631Z",
     "iopub.execute_input": "2024-10-04T13:23:06.502005Z",
     "iopub.status.idle": "2024-10-04T13:26:54.123332Z",
     "shell.execute_reply.started": "2024-10-04T13:23:06.501978Z",
     "shell.execute_reply": "2024-10-04T13:26:54.122406Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.19903595810301783\ncurrent accuracy is  83.48214285714286\nEpoch 0/0, Loss: 0.16467952580622783\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.1494975252781154\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.13664636298696078\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.12710114868312347\ncurrent accuracy is  83.66071428571429\nEpoch 0/0, Loss: 0.11853706113221806\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.11903342135000701\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.11633001786951976\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.10985988486989788\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.09698048030117118\ncurrent accuracy is  85.625\nCurrent accuracy is: 86.17%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.66      0.68      0.67        34\n         mel       0.90      0.88      0.89        49\n         bkl       0.74      0.73      0.74       109\n         bcc       0.83      0.45      0.59        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.49      0.71      0.58        92\n          df       0.60      1.00      0.75        15\n\n    accuracy                           0.86      1121\n   macro avg       0.74      0.77      0.73      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 23   0   4   1   1   4   1]\n [  2  43   0   0   2   1   1]\n [  6   0  80   0  10  13   0]\n [  0   2   2   5   1   0   1]\n [  2   2  16   0 735  51   5]\n [  2   1   6   0  16  65   2]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher6', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:28:05.041264Z",
     "iopub.execute_input": "2024-10-04T13:28:05.041758Z",
     "iopub.status.idle": "2024-10-04T13:31:52.541116Z",
     "shell.execute_reply.started": "2024-10-04T13:28:05.041726Z",
     "shell.execute_reply": "2024-10-04T13:31:52.540140Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.2082034497090229\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.17687453118311922\ncurrent accuracy is  83.83928571428571\nEpoch 0/0, Loss: 0.15671020642876987\ncurrent accuracy is  84.28571428571429\nEpoch 0/0, Loss: 0.1350995908866114\ncurrent accuracy is  83.39285714285714\nEpoch 0/0, Loss: 0.12499034403051\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.1334914527379158\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.11741352687226381\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.10760436429664856\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.1005431971649026\ncurrent accuracy is  84.64285714285714\nEpoch 0/0, Loss: 0.11267265562740404\ncurrent accuracy is  85.53571428571429\nCurrent accuracy is: 86.80%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.71      0.68        34\n         mel       0.89      0.86      0.88        49\n         bkl       0.71      0.71      0.71       109\n         bcc       0.62      0.73      0.67        11\n       akiec       0.96      0.92      0.94       811\n        vasc       0.54      0.68      0.61        92\n          df       0.71      1.00      0.83        15\n\n    accuracy                           0.87      1121\n   macro avg       0.73      0.80      0.76      1121\nweighted avg       0.88      0.87      0.87      1121\n\nConfusion Matrix:\n [[ 24   0   5   1   1   2   1]\n [  3  42   0   0   3   0   1]\n [  6   1  77   2  12  11   0]\n [  1   1   0   8   1   0   0]\n [  1   2  20   0 744  40   4]\n [  2   1   6   2  18  63   0]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher7', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:35:09.286475Z",
     "iopub.execute_input": "2024-10-04T13:35:09.287361Z",
     "iopub.status.idle": "2024-10-04T13:38:54.156568Z",
     "shell.execute_reply.started": "2024-10-04T13:35:09.287328Z",
     "shell.execute_reply": "2024-10-04T13:38:54.155552Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.18595268589830616\ncurrent accuracy is  83.66071428571429\nEpoch 0/0, Loss: 0.171760114511793\ncurrent accuracy is  83.75\nEpoch 0/0, Loss: 0.14679295164192233\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.14102411340558674\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.13073956095272812\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.1182481561355838\ncurrent accuracy is  84.46428571428571\nEpoch 0/0, Loss: 0.11496451535693757\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.11344903016962656\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.09496678803797538\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.08145791851551976\ncurrent accuracy is  84.19642857142857\nCurrent accuracy is: 85.01%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.61      0.74      0.67        34\n         mel       0.88      0.86      0.87        49\n         bkl       0.68      0.73      0.71       109\n         bcc       0.71      0.45      0.56        11\n       akiec       0.97      0.89      0.93       811\n        vasc       0.46      0.71      0.56        92\n          df       0.65      1.00      0.79        15\n\n    accuracy                           0.85      1121\n   macro avg       0.71      0.77      0.72      1121\nweighted avg       0.88      0.85      0.86      1121\n\nConfusion Matrix:\n [[ 25   0   5   1   0   2   1]\n [  3  42   1   0   2   0   1]\n [  5   1  80   0   6  17   0]\n [  2   2   1   5   1   0   0]\n [  4   2  23   0 721  56   5]\n [  2   1   7   1  15  65   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher8', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:40:59.540689Z",
     "iopub.execute_input": "2024-10-04T13:40:59.541453Z",
     "iopub.status.idle": "2024-10-04T13:44:47.178959Z",
     "shell.execute_reply.started": "2024-10-04T13:40:59.541418Z",
     "shell.execute_reply": "2024-10-04T13:44:47.177829Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.1944242732644808\ncurrent accuracy is  84.375\nEpoch 0/0, Loss: 0.16791856162840638\ncurrent accuracy is  83.83928571428571\nEpoch 0/0, Loss: 0.15872548299091982\ncurrent accuracy is  83.48214285714286\nEpoch 0/0, Loss: 0.13683192486443171\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.12494328941741004\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.12274455950913452\ncurrent accuracy is  84.82142857142857\nEpoch 0/0, Loss: 0.12013699090471719\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.11407167959117853\ncurrent accuracy is  85.17857142857143\nEpoch 0/0, Loss: 0.09513375649751141\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.1043483610037805\ncurrent accuracy is  85.625\nCurrent accuracy is: 86.71%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.70      0.68      0.69        34\n         mel       0.89      0.86      0.88        49\n         bkl       0.69      0.75      0.72       109\n         bcc       1.00      0.64      0.78        11\n       akiec       0.96      0.91      0.94       811\n        vasc       0.52      0.67      0.58        92\n          df       0.68      1.00      0.81        15\n\n    accuracy                           0.87      1121\n   macro avg       0.78      0.79      0.77      1121\nweighted avg       0.88      0.87      0.87      1121\n\nConfusion Matrix:\n [[ 23   0   5   0   1   4   1]\n [  3  42   1   0   2   0   1]\n [  3   1  82   0  10  13   0]\n [  1   1   1   7   1   0   0]\n [  1   2  22   0 741  41   4]\n [  2   1   7   0  19  62   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher9', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:45:18.507817Z",
     "iopub.execute_input": "2024-10-04T13:45:18.508465Z",
     "iopub.status.idle": "2024-10-04T13:49:05.967167Z",
     "shell.execute_reply.started": "2024-10-04T13:45:18.508431Z",
     "shell.execute_reply": "2024-10-04T13:49:05.966250Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.18779352956973924\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.16079810862543015\ncurrent accuracy is  85.26785714285714\nEpoch 0/0, Loss: 0.147086153840419\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.1338492687791586\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.12872069380132528\ncurrent accuracy is  86.16071428571429\nEpoch 0/0, Loss: 0.11622010535424257\ncurrent accuracy is  83.92857142857143\nEpoch 0/0, Loss: 0.1129714723040418\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.11978563808313594\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.10649579064352666\ncurrent accuracy is  85.625\nEpoch 0/0, Loss: 0.09914914783226644\ncurrent accuracy is  85.26785714285714\nCurrent accuracy is: 86.08%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.91      0.84      0.87        49\n         bkl       0.70      0.75      0.73       109\n         bcc       0.83      0.45      0.59        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.50      0.72      0.59        92\n          df       0.65      1.00      0.79        15\n\n    accuracy                           0.86      1121\n   macro avg       0.74      0.76      0.73      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 22   0   5   0   1   5   1]\n [  3  41   1   0   3   0   1]\n [  5   1  82   0  10  11   0]\n [  1   1   2   5   1   0   1]\n [  1   1  21   0 734  50   4]\n [  2   1   6   1  15  66   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "retrain_model_eff(name='watcher10', layid = 3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:50:01.215785Z",
     "iopub.execute_input": "2024-10-04T13:50:01.216163Z",
     "iopub.status.idle": "2024-10-04T13:53:48.759075Z",
     "shell.execute_reply.started": "2024-10-04T13:50:01.216133Z",
     "shell.execute_reply": "2024-10-04T13:53:48.758153Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total number of batches in Train Loader: 164\nTotal number of batches in Val Loader: 35\nCurrent accuracy is: 85.91%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.65      0.65      0.65        34\n         mel       0.86      0.86      0.86        49\n         bkl       0.68      0.73      0.70       109\n         bcc       1.00      0.27      0.43        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.48      0.67      0.56        92\n          df       0.83      1.00      0.91        15\n\n    accuracy                           0.86      1121\n   macro avg       0.78      0.73      0.72      1121\nweighted avg       0.88      0.86      0.86      1121\n\nConfusion Matrix:\n [[ 22   0   6   0   1   5   0]\n [  3  42   1   0   2   0   1]\n [  3   1  80   0  11  14   0]\n [  1   2   2   3   2   1   0]\n [  1   1  22   0 739  46   2]\n [  4   3   7   0  16  62   0]\n [  0   0   0   0   0   0  15]]\nEpoch 0/0, Loss: 0.1978333115509552\ncurrent accuracy is  83.48214285714286\nEpoch 0/0, Loss: 0.1653913461330642\ncurrent accuracy is  84.10714285714286\nEpoch 0/0, Loss: 0.1570060642368001\ncurrent accuracy is  85.26785714285714\nEpoch 0/0, Loss: 0.14835779217821432\ncurrent accuracy is  84.73214285714286\nEpoch 0/0, Loss: 0.12488758213045757\ncurrent accuracy is  85.35714285714286\nEpoch 0/0, Loss: 0.12193890035197866\ncurrent accuracy is  84.55357142857143\nEpoch 0/0, Loss: 0.10959340000479686\ncurrent accuracy is  85.08928571428571\nEpoch 0/0, Loss: 0.10040813325563581\ncurrent accuracy is  85.0\nEpoch 0/0, Loss: 0.12285139578076579\ncurrent accuracy is  84.91071428571429\nEpoch 0/0, Loss: 0.09699015645878162\ncurrent accuracy is  85.44642857142857\nCurrent accuracy is: 86.44%\nClassification Report:\n               precision    recall  f1-score   support\n\n          nv       0.72      0.68      0.70        34\n         mel       0.86      0.88      0.87        49\n         bkl       0.70      0.77      0.73       109\n         bcc       0.75      0.55      0.63        11\n       akiec       0.96      0.91      0.93       811\n        vasc       0.50      0.68      0.58        92\n          df       0.68      1.00      0.81        15\n\n    accuracy                           0.86      1121\n   macro avg       0.74      0.78      0.75      1121\nweighted avg       0.88      0.86      0.87      1121\n\nConfusion Matrix:\n [[ 23   0   5   1   1   3   1]\n [  2  43   1   0   2   0   1]\n [  5   0  84   0   7  13   0]\n [  0   3   0   6   1   0   1]\n [  0   3  23   0 735  47   3]\n [  2   1   7   1  17  63   1]\n [  0   0   0   0   0   0  15]]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "\n    \nlayer_replacement_info = {\n        'net.features.6.0.block.2.fc1': 'over-trained',\n        'net.features.6.0.block.2.fc2': 'over-trained',\n        'net.features.2.0.block.3.0': 'under-trained',\n        'net.features.3.1.block.3.0': 'under-trained',\n        'net.features.6.0.block.3.0': 'under-trained',\n        'net.features.7.0.block.2.fc1': 'under-trained',\n        'net.features.7.0.block.3.0': 'under-trained',\n    }\n\n\n\n\ndef main_eff_retrain(model = None, name='test',params_to_tune=None):\n    # Hyperparameters\n    num_epochs_mlp = 1\n    num_epochs_total = 10  # Total epochs including both MLP and U-Net\n    learning_rate = 0.001\n    BATCH_SIZE = 32\n\n    # Now you can train the model on your dataset\n    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n#   this is for tune only, 1 and 2, the follows is only tune \n      \n    \n#     optimizer = optim.Adam(model.net.features.7.0.block.2.fc.parameters(), lr=1e-4)\n    optimizer = optim.Adam(params_to_tune, lr=1e-4)\n    from torch.utils.data import DataLoader, WeightedRandomSampler\n    from collections import Counter\n    # Compute class weights for imbalanced dataset\n    class_counts = Counter([label for label in train_df['cell_type_idx']])\n    class_weights = {class_id: 1.0 / count for class_id, count in class_counts.items()}\n    sample_weights = [class_weights[label] for label in train_df['cell_type_idx']]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n    \n\n    image_source = '/kaggle/input/skin-cancer-mnist-ham10000/'\n    train_data = dataMedicalImage(train_df, image_folder= image_source, transform=transform_train)\n    val_data = dataMedicalImage(val_df,image_folder= image_source,  transform=transform_val)\n    test_data = dataMedicalImage(test_df, image_folder= image_source, transform=transform_val)\n    \n    # Data loaders\n    trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=sampler)\n    valloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n    testloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    print(f\"Total number of batches in Train Loader: {len(trainloader)}\")\n    print(f\"Total number of batches in Val Loader: {len(valloader)}\")\n\n    best_model = None\n    accf = 0\n    test_model(model, testloader)\n    \n    for epoch in range(0, num_epochs_total, 1):\n        # Train MLP for 10 epochs\n        model = train_model(model, trainloader, criterion, optimizer, num_epochs_mlp)\n\n        acc = eval_model(model, valloader)\n        if acc > accf:\n            accf = acc\n            best_model = model\n            torch.save(best_model.state_dict(), 'retrain_eff_old_model_'+name+'.pt')\n    test_model(best_model, testloader)\n    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:17:52.411247Z",
     "iopub.execute_input": "2024-10-04T13:17:52.411651Z",
     "iopub.status.idle": "2024-10-04T13:17:52.424606Z",
     "shell.execute_reply.started": "2024-10-04T13:17:52.411618Z",
     "shell.execute_reply": "2024-10-04T13:17:52.423667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch import nn\nfrom torchvision import models\n\n\ndef change_parameters(model1, model2):\n    # Load model1 with pre-trained weights\n    # model1 = models.efficientnet_b0(weights='DEFAULT')\n\n    # # Load model2 with custom weights\n    # model2 = models.efficientnet_b0(weights=None)  # Initialize with no pre-trained weights\n    # model2.load_state_dict(torch.load('/eff_combine_model_tune.pt'))\n\n    # Define the layers to replace and the layers to tune or freeze\n    layer_replacement_info = {\n        'net.features.6.0.block.2.fc1': 'over-trained',\n        'net.features.6.0.block.2.fc2': 'over-trained',\n        'net.features.2.0.block.3.0': 'under-trained',\n        'net.features.3.1.block.3.0': 'under-trained',\n        'net.features.6.0.block.3.0': 'under-trained',\n        'net.features.7.0.block.2.fc1': 'under-trained',\n        'net.features.7.0.block.3.0': 'under-trained',\n    }\n\n    # Helper function to get submodule by name\n    def get_submodule_by_name(model, name):\n        for n, m in model.named_modules():\n            if n == name:\n#                 print('--',n)\n                return m\n        return None\n\n    # Replace weights and set parameters\n    for layer_name, status in layer_replacement_info.items():\n        # Remove 'net.' prefix for module lookup\n#         cleaned_layer_name = layer_name.replace('net.', '')\n\n        # Get the layers from both models\n        layer_model1 = get_submodule_by_name(model1, layer_name)\n        layer_model2 = get_submodule_by_name(model2, layer_name)\n#         print(layer_model1)\n#         print(layer_model2)\n        if status == 'over-trained':\n            # Replace the weights of model2 with model1's weights for over-trained layers\n            if layer_model1 is not None and layer_model2 is not None:\n                layer_model2.load_state_dict(layer_model1.state_dict())\n            # Freeze the parameters of the over-trained layers in model2\n            for param in layer_model2.parameters():\n                param.requires_grad = True\n        elif status == 'under-trained':\n            # Make sure the parameters are trainable for under-trained layers in model2\n            if layer_model2 is not None:\n                for param in layer_model2.parameters():\n                    param.requires_grad = True\n\n    # Now model2 has weights replaced for over-trained layers and appropriate layers frozen or unfrozen.\n    return model2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T13:17:48.434843Z",
     "iopub.execute_input": "2024-10-04T13:17:48.435717Z",
     "iopub.status.idle": "2024-10-04T13:17:48.445640Z",
     "shell.execute_reply.started": "2024-10-04T13:17:48.435685Z",
     "shell.execute_reply": "2024-10-04T13:17:48.444740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "model_eff = change_parameters(model1, model2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T10:00:22.186057Z",
     "iopub.execute_input": "2024-10-04T10:00:22.186428Z",
     "iopub.status.idle": "2024-10-04T10:00:22.200542Z",
     "shell.execute_reply.started": "2024-10-04T10:00:22.186388Z",
     "shell.execute_reply": "2024-10-04T10:00:22.199553Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "-- net.features.6.0.block.2.fc1\n-- net.features.6.0.block.2.fc1\nConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\nConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n-- net.features.6.0.block.2.fc2\n-- net.features.6.0.block.2.fc2\nConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\nConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n-- net.features.2.0.block.3.0\n-- net.features.2.0.block.3.0\nConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\nConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n-- net.features.3.1.block.3.0\n-- net.features.3.1.block.3.0\nConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\nConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n-- net.features.6.0.block.3.0\n-- net.features.6.0.block.3.0\nConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\nConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n-- net.features.7.0.block.2.fc1\n-- net.features.7.0.block.2.fc1\nConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\nConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n-- net.features.7.0.block.3.0\n-- net.features.7.0.block.3.0\nConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\nConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": "# pie-chart plots- (\nAssignment 2 or skin-cancer-pie-chart)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install torchinfo\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom torch.utils.data import Dataset, WeightedRandomSampler, DataLoader, Subset\nfrom transformers import ConvNextForImageClassification\nfrom torchvision.transforms.functional import to_pil_image\nfrom torchvision import transforms\nfrom torchinfo import summary\nfrom PIL import Image\n",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Define a function to combine images in different folders into 1 folders\ndef combine_folders(src1, src2, dest):\n\n    copied_files = []  # List to store names of successfully copied files\n\n    # Create folder when destination folder not exist, exit function if destination folder contains contents from the source folder\n    if not os.path.exists(dest):\n      print(f\"Destination folder {dest} created...\")\n      os.makedirs(dest)\n\n    else:\n      # Recreate folder when destination folder do not have all the contents in source folders to avoid overlapping contents\n      if len(os.listdir(dest)) == (len(os.listdir(src1)) + len(os.listdir(src2))):\n        print(\"Content in source 1 folder and source 2 folder have been copied to destination folder\")\n\n        for filename in os.listdir(dest):\n            copied_files.append(filename)\n\n        return copied_files\n\n      else:\n        shutil.rmtree(dest)\n        os.makedirs(dest)\n\n\n    print(f\"Copying images in {src1} into {dest}...\")\n\n    # Loop through all content in source folder 1 and copy into destination folder, if source 1 folder exist\n    if os.path.exists(src1):\n      print('Source 1 folder consist of ', len(os.listdir(src1)), \" elements\")\n\n      for images in os.listdir(src1):\n\n        # Define the source path and the destination path for each images\n        src_path = os.path.join(src1, images)\n        dest_path = os.path.join(dest, images)\n\n        # Copy to the destination folder\n        shutil.copy(src_path, dest_path)\n        copied_files.append(images)\n\n      print(f\"All images in {src1} have been copied into {dest}...\")\n\n    else:\n      print(f\"{src1} not found\")\n\n\n    print(f\"Copying images in {src2} into {dest}...\")\n\n    # loop through all content in source folder 2 and copy into destination folder, if source 2 folder exist\n    if os.path.exists(src2):\n      print('Source 2 folder consist of ', len(os.listdir(src2)), \" elements\")\n\n      for images in os.listdir(src2):\n\n        # Define the source path and the destination path for each images\n        src_path = os.path.join(src2, images)\n        dest_path = os.path.join(dest, images)\n\n        # Copy to the destination folder\n        shutil.copy(src_path, dest_path)\n        copied_files.append(images)\n\n      print(f\"All images in {src2} have been copied into {dest}...\")\n\n    else:\n      print(f\"{src2} not found\")\n\n    print('Destination folder consist of ', len(os.listdir(dest)), \" elements\")\n\n    return copied_files",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Source directory\nimport shutil\n\nsource_dir = \"/kaggle/input/skin-cancer-mnist-ham10000\"\n\n# Destination directory\ndestination_dir = \"/kaggle/working/Dataset\"\n\nshutil.copytree(source_dir, destination_dir)\n# Define source paths to combined and destination path to store\nsrc1 = \"/kaggle/working/Dataset/HAM10000_images_part_1/\"\nsrc2 = \"/kaggle/working/Dataset/HAM10000_images_part_2/\"\ndest = \"/kaggle/working/Dataset/HAM10000/\"\ncopied_files = combine_folders(src1, src2, dest)",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_file = \"Dataset/HAM10000_metadata.csv\"\nskin_train_df = pd.read_csv(train_file)\n\nskin_train_df['image_id_with_extension'] = skin_train_df['image_id'] + '.jpg'\nskin_train_df = skin_train_df[skin_train_df['image_id_with_extension'].isin(copied_files)]\nskin_train_df = skin_train_df.drop(columns=['image_id_with_extension'])\n\nskin_train_df.head(5)",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Dictionary to map lesion type codes to their full names\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Filter out the 'nv' category if it exists\nfiltered_df = skin_train_df[skin_train_df['dx'] != 'nv']\n\n\nimport matplotlib.pyplot as plt\n# Group by 'dx' and 'localization' and then count the occurrences\ngrouped_counts = filtered_df.groupby(['localization', 'dx']).size().reset_index(name='count')\n\n# Get unique localization categories\nlocalizations = grouped_counts['localization'].unique()\n# Define a fixed color map for each lesion type (dx)\ncolor_map = {\n    'mel': '#1f77b4',  # Blue\n    'bkl': '#ff7f0e',  # Orange\n    'bcc': '#2ca02c',  # Green\n    'akiec': '#d62728',  # Red\n    'vasc': '#9467bd',  # Purple\n    'df': '#8c564b',    # Brown\n    'nv': '#8c7e4b',\n}\n\n# Loop over each localization and save each pie chart as a separate PNG file\nfor i, localization in enumerate(localizations):\n    # Filter data for the current localization\n    loc_data = grouped_counts[grouped_counts['localization'] == localization]\n    \n    # Map the 'dx' values using the lesion_type_dict for the labels\n    labels = loc_data['dx'].map(lesion_type_dict)\n    \n    # Create a list of colors for the current dx values using the fixed color_map\n    colors = [color_map[dx] for dx in loc_data['dx']]\n    print(colors, loc_data)\n    # Create a new figure for each pie chart\n    fig, ax = plt.subplots(figsize=(6, 4))\n    \n    # Pie chart without labels, using the predefined colors\n    wedges, texts, autotexts = ax.pie(\n        loc_data['count'], autopct='%1.1f%%', startangle=90, colors=colors\n    )\n    \n    # Set title for the pie chart\n    ax.set_title(f'Lesion Type-[{localization.capitalize()}]')\n    \n    # Ensure the pie is drawn as a circle\n    ax.set_aspect('equal')\n\n    # Create a custom legend\n    legend_entries = [f\"{label}: {count}\" for label, count in zip(labels, loc_data['count'])]\n    \n    # Display the legend outside the chart\n    ax.legend(wedges, legend_entries, title=\"Lesion Type\", loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=14, frameon=False)\n    \n    # Save the figure as a PNG file\n    plt.savefig(f'pie_chart_{localization}.png', bbox_inches='tight')\n    \n    # Close the figure to free up memory\n    plt.close()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "skin_train_df['localization'].unique()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "download the test folder data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_df = pd.read_csv('/kaggle/input/ucidatasplits/test_df.csv').drop(columns=['Unnamed: 0'])\ntest_df.head()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-20T14:26:01.761929Z",
     "iopub.execute_input": "2025-01-20T14:26:01.762287Z",
     "iopub.status.idle": "2025-01-20T14:26:01.788928Z",
     "shell.execute_reply.started": "2025-01-20T14:26:01.762261Z",
     "shell.execute_reply": "2025-01-20T14:26:01.787927Z"
    }
   },
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "     lesion_id      image_id   dx    dx_type   age     sex     localization  \\\n0  HAM_0001359  ISIC_0028628  bkl      histo  75.0    male  lower extremity   \n1  HAM_0003188  ISIC_0030765   nv      histo  30.0  female          abdomen   \n2  HAM_0005275  ISIC_0028749  bkl      histo  65.0    male             face   \n3  HAM_0001061  ISIC_0029526   nv  follow_up  45.0    male             back   \n4  HAM_0003249  ISIC_0025114   nv  follow_up  35.0    male             back   \n\n                                           path  \\\n0  data\\HAM10000_images_part_1\\ISIC_0028628.jpg   \n1  data\\HAM10000_images_part_2\\ISIC_0030765.jpg   \n2  data\\HAM10000_images_part_1\\ISIC_0028749.jpg   \n3  data\\HAM10000_images_part_2\\ISIC_0029526.jpg   \n4  data\\HAM10000_images_part_1\\ISIC_0025114.jpg   \n\n                        cell_type  cell_type_idx  \n0  Benign keratosis-like lesions               2  \n1                Melanocytic nevi              4  \n2  Benign keratosis-like lesions               2  \n3                Melanocytic nevi              4  \n4                Melanocytic nevi              4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0001359</td>\n      <td>ISIC_0028628</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n      <td>data\\HAM10000_images_part_1\\ISIC_0028628.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0003188</td>\n      <td>ISIC_0030765</td>\n      <td>nv</td>\n      <td>histo</td>\n      <td>30.0</td>\n      <td>female</td>\n      <td>abdomen</td>\n      <td>data\\HAM10000_images_part_2\\ISIC_0030765.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0005275</td>\n      <td>ISIC_0028749</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>face</td>\n      <td>data\\HAM10000_images_part_1\\ISIC_0028749.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0001061</td>\n      <td>ISIC_0029526</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>45.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>data\\HAM10000_images_part_2\\ISIC_0029526.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0003249</td>\n      <td>ISIC_0025114</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>35.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>data\\HAM10000_images_part_1\\ISIC_0025114.jpg</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "test_df['image'] = test_df['path'].map(lambda x: (x[5:27]+'/'+x[-16:]))\ntest_df.head(1) ",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-20T14:33:23.471529Z",
     "iopub.execute_input": "2025-01-20T14:33:23.472006Z",
     "iopub.status.idle": "2025-01-20T14:33:23.491114Z",
     "shell.execute_reply.started": "2025-01-20T14:33:23.471972Z",
     "shell.execute_reply": "2025-01-20T14:33:23.489946Z"
    }
   },
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "     lesion_id      image_id   dx dx_type   age   sex     localization  \\\n0  HAM_0001359  ISIC_0028628  bkl   histo  75.0  male  lower extremity   \n\n                                           path  \\\n0  data\\HAM10000_images_part_1\\ISIC_0028628.jpg   \n\n                        cell_type  cell_type_idx  \\\n0  Benign keratosis-like lesions               2   \n\n                                     image  \n0  HAM10000_images_part_1/ISIC_0028628.jpg  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0001359</td>\n      <td>ISIC_0028628</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n      <td>data\\HAM10000_images_part_1\\ISIC_0028628.jpg</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>HAM10000_images_part_1/ISIC_0028628.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-20T14:34:32.892749Z",
     "iopub.execute_input": "2025-01-20T14:34:32.893488Z",
     "iopub.status.idle": "2025-01-20T14:34:32.898050Z",
     "shell.execute_reply.started": "2025-01-20T14:34:32.893461Z",
     "shell.execute_reply": "2025-01-20T14:34:32.897104Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "import os\nimport shutil\n\n# Define source and destination directories\nsource_folder = \"/kaggle/input/skin-cancer-mnist-ham10000/\"\ndestination_folder = \"/kaggle/working/testdata/\"\n\n# Ensure destination folder exists\nos.makedirs(destination_folder, exist_ok=True)\n\n# Iterate through the 'image' column in result_df\nfor image_name in test_df['image']:\n    # Construct full file paths\n    source_path = os.path.join(source_folder, image_name)  # Assuming files have .jpg extension\n    destination_path = os.path.join(destination_folder, image_name[-16:])\n    \n    # Copy file if it exists in the source folder\n    if os.path.exists(source_path):\n        shutil.copy(source_path, destination_path)\n    else:\n        print(f\"File not found: {source_path}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-20T14:35:09.042935Z",
     "iopub.execute_input": "2025-01-20T14:35:09.043814Z",
     "iopub.status.idle": "2025-01-20T14:35:22.048205Z",
     "shell.execute_reply.started": "2025-01-20T14:35:09.043787Z",
     "shell.execute_reply": "2025-01-20T14:35:22.047357Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "# Create a zip archive\noutput_zip_file = \"to_testfolder.zip\"\nshutil.make_archive(base_name=output_zip_file.replace('.zip', ''), format='zip', root_dir='/kaggle/working/testdata')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-20T14:50:57.147131Z",
     "iopub.execute_input": "2025-01-20T14:50:57.147512Z",
     "iopub.status.idle": "2025-01-20T14:51:08.979491Z",
     "shell.execute_reply.started": "2025-01-20T14:50:57.147483Z",
     "shell.execute_reply": "2025-01-20T14:51:08.978218Z"
    }
   },
   "outputs": [
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/kaggle/working/to_testfolder.zip'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
